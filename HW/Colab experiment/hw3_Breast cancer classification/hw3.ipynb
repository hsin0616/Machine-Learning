{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Week 4: Colab Experiment**"],"metadata":{"id":"4ZlU8khsZjZT"}},{"cell_type":"markdown","source":["# I. Introduction\n","In this exercise, we load the Breast cancer wisconsin dataset for classification."],"metadata":{"id":"E3LRo3ehZo2B"}},{"cell_type":"markdown","source":["# II. Methods"],"metadata":{"id":"Sn2Bcr9sZofG"}},{"cell_type":"code","execution_count":18,"metadata":{"id":"X4dRDQZqqiet","executionInfo":{"status":"ok","timestamp":1727955157751,"user_tz":-480,"elapsed":550,"user":{"displayName":"Hsin-En Tsai","userId":"01830060851848219976"}}},"outputs":[],"source":["from sklearn.datasets import load_breast_cancer\n","import pandas as pd\n","from collections import Counter\n","from datetime import datetime\n","import numpy as np\n","from sklearn.model_selection import KFold\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import zero_one_loss\n"]},{"cell_type":"code","source":["# Define the dependent and independent variables.\n","data = load_breast_cancer()\n","Y = data.target\n","X = data.data"],"metadata":{"id":"ArV6oId2qjCh","executionInfo":{"status":"ok","timestamp":1727955158164,"user_tz":-480,"elapsed":10,"user":{"displayName":"Hsin-En Tsai","userId":"01830060851848219976"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Create CV folds\n","num_folds = 5\n","kf = KFold(n_splits=num_folds, random_state=0, shuffle=True)\n","kfold_indices = {}\n","\n","for i, (train_index, test_index) in enumerate(kf.split(X)):\n","  kfold_indices[f\"fold_{i}\"] = {'train': train_index, 'test': test_index}"],"metadata":{"id":"_kY6lUBXL0TX","executionInfo":{"status":"ok","timestamp":1727955158164,"user_tz":-480,"elapsed":9,"user":{"displayName":"Hsin-En Tsai","userId":"01830060851848219976"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# Train models and apply them to the test set\n","\n","Error_rate = {'logreg': [], 'svm': [], 'decision_tree': []}\n","\n","scaler = StandardScaler()  # 標準化，避免不同範圍特徵對正則化造成影響\n","\n","for fold_id in range(num_folds):\n","  X_train = X[kfold_indices[f\"fold_{fold_id}\"]['train']]\n","  Y_train = Y[kfold_indices[f\"fold_{fold_id}\"]['train']]\n","  X_test = X[kfold_indices[f\"fold_{fold_id}\"]['test']]\n","  Y_test = Y[kfold_indices[f\"fold_{fold_id}\"]['test']]\n","\n","  X_train_scaled = scaler.fit_transform(X_train)\n","  X_test_scaled = scaler.transform(X_test)\n","\n","  # Logistic regression\n","  ######################## TODO #####################################\n","  logreg = LogisticRegression(max_iter=10000, solver='liblinear')\n","\n","  param_grid_logreg = {\n","    'C': [0.01, 0.1, 1, 10, 100],  # 正則化參數\n","    'penalty': ['l1', 'l2']        # 正則化類型\n","    }\n","\n","  grid_logreg = GridSearchCV(logreg, param_grid_logreg, cv=5, scoring='accuracy')\n","  grid_logreg.fit(X_train_scaled, Y_train)\n","  best_logreg = grid_logreg.best_estimator_\n","  Y_pred_logreg = best_logreg.predict(X_test_scaled)\n","  error_logreg = zero_one_loss(Y_test, Y_pred_logreg)\n","  Error_rate['logreg'].append(error_logreg)\n","  #####################################################################\n","\n","\n","  # SVM\n","  ######################## TODO #####################################\n","  svm = SVC(kernel='linear')\n","\n","  param_grid_svm = {\n","    'C': [0.1, 1, 10, 100],          # 惩罰參數\n","    'kernel': ['linear', 'rbf'],     # 核函數類型\n","    # 'gamma': ['scale', 'auto']        # 核係數\n","  }\n","  grid_svm = GridSearchCV(svm, param_grid_svm, cv=5, scoring='accuracy')\n","  grid_svm.fit(X_train_scaled, Y_train)\n","  best_svm = grid_svm.best_estimator_\n","  Y_pred_svm = best_svm.predict(X_test_scaled)\n","  error_svm = zero_one_loss(Y_test, Y_pred_svm)\n","  Error_rate['svm'].append(error_svm)\n","  #####################################################################\n","\n","\n","  # Decision tree\n","  ######################## TODO #####################################\n","  dt = DecisionTreeClassifier(random_state=0)\n","\n","  param_grid_dt = {\n","    'max_depth': [None, 5, 10, 20, 30],  # 樹的最大深度\n","    'min_samples_split': [2, 5, 10],      # 分裂所需的最小樣本數\n","    'min_samples_leaf': [1, 2, 4]         # 葉節點的最小樣本數\n","  }\n","  grid_dt = GridSearchCV(dt, param_grid_dt, cv=5, scoring='accuracy')\n","  grid_dt.fit(X_train_scaled, Y_train)\n","  best_dt = grid_dt.best_estimator_\n","  Y_pred_dt = best_dt.predict(X_test_scaled)\n","  error_dt = zero_one_loss(Y_test, Y_pred_dt)\n","  Error_rate['decision_tree'].append(error_dt)\n","  #####################################################################"],"metadata":{"id":"UsTfhZNxL0V1","executionInfo":{"status":"ok","timestamp":1727955171743,"user_tz":-480,"elapsed":13587,"user":{"displayName":"Hsin-En Tsai","userId":"01830060851848219976"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["## III. Results\n","\n","Here we report the mean and standard deviation of the error rates over 5 folds for each method."],"metadata":{"id":"tW0uMLYwZ63z"}},{"cell_type":"code","source":["######################## TODO #####################################\n","print(f\"The error rate over 5 folds in CV:\")\n","\n","for model in Error_rate:\n","  avg_error = np.mean(Error_rate[model])\n","  std_error = np.std(Error_rate[model])\n","  print(f\"{model}: mean = {avg_error}, std = {std_error}\")\n","#####################################################################\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0uD1iPyJP25T","outputId":"00a84871-309a-4e3d-edfb-78a98c7b1cb3","executionInfo":{"status":"ok","timestamp":1727955171743,"user_tz":-480,"elapsed":19,"user":{"displayName":"Hsin-En Tsai","userId":"01830060851848219976"}}},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["The error rate over 5 folds in CV:\n","logreg: mean = 0.02105263157894739, std = 0.01312862240973312\n","svm: mean = 0.026315789473684202, std = 0.014678246079545192\n","decision_tree: mean = 0.059711224965067554, std = 0.014990903335492592\n"]}]},{"cell_type":"markdown","source":["# IV. Conclusion and Discussion\n"],"metadata":{"id":"srwwVH9TaBm3"}},{"cell_type":"markdown","source":["　　這次作業利用Logistic Regression, SVM, Decision tree等三種模型，並以 Error Rate 作為評估指標。以下將對parameter 的選擇、實驗結果及其與研究目的的關聯進行討論。\n","\n","**參數選擇的影響分析**\n","\n","1. Logistic Regression\n","\n","　　正則化參數 C 和正規化類型（L1、L2）的選擇會影響模型的泛化能力和特徵選擇。較小的 C 值會強化正則化，防止過擬合。\n","\n","2. SVM\n","\n","　　Penalty parameter C 和核函數類型（linear、rbf）對模型性能影響顯著。不同的 C 值會影響分類與模型複雜度，選擇適當的核函數則會決定模型是否能得到資料的非線性特徵。\n","\n","3. Decision Tree\n","\n","　　最大深度（max_depth）、分裂最小樣本數（min_samples_split）及 leave nodes 最小樣本數（min_samples_leaf）等參數控制樹的複雜度，直接影響模型的過擬合與泛化能力。\n","\n","**實驗結果分析**\n","\n","*   Logistic Regression：mean error 為 0.021、標準差為 0.013。\n","*   SVM：mean error 為 0.026、標準差為 0.015。\n","*   Decision Tree：mean error 為 0.060、標準差為 0.015。\n","\n","　　從結果可看出 logistic regression 在這次實驗的結果最佳，有最低的 mean error 和相對較小的標準差，代表其穩定性較高。SVM 次之，雖然 mean error 稍高於 logistic regression，但也有不錯的結果，表示它也具良好的分類能力。Decision Tree 的 mean error 明顯較高，且標準差與 SVM 相近，表示它在本次資料集上的表現較為不穩定，可能是因為此模型較容易過擬合。\n","\n","**預測效果與目的**\n","\n","　　本次作業的主要目的是建立有效的模型來預測乳癌的分類，以提升早期診斷的準確性，並改善治療效果。Logisic regression 和 SVM 在 mean error 上有較好表現，表示這兩種方法在乳癌分類中具有較高的準確性。Decision tree 具較高 mean error，這限制它在此應用中的實用性，需進一步調整參數或使用 Random Forests 以提升性能，才能更好地應用於乳癌分類。"],"metadata":{"id":"HlD7UuvP-jN6"}}]}