{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n44rMWcLS-BY"
   },
   "source": [
    "# Week 10: Colab Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCMvaDkMTJkT"
   },
   "source": [
    "# I. Introduction\n",
    "In this exercise, we apply CNN to MNIST data to classify the hand written digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 7729,
     "status": "ok",
     "timestamp": 1730966515403,
     "user": {
      "displayName": "Hsin-En Tsai",
      "userId": "01830060851848219976"
     },
     "user_tz": -480
    },
    "id": "2jxq00nbuCwt"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdXkw_9pkfn5"
   },
   "source": [
    "# Data Loading\n",
    "Load the data from the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1730966515403,
     "user": {
      "displayName": "Hsin-En Tsai",
      "userId": "01830060851848219976"
     },
     "user_tz": -480
    },
    "id": "PoUAesyDuL0n"
   },
   "outputs": [],
   "source": [
    "# Run this once to load the train and test data straight into a dataloader class\n",
    "# that will provide the batches\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRXOY0Tzkfn6"
   },
   "source": [
    "# Visualize dataset sample\n",
    "Show some sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "executionInfo": {
     "elapsed": 2403,
     "status": "ok",
     "timestamp": 1730966517800,
     "user": {
      "displayName": "Hsin-En Tsai",
      "userId": "01830060851848219976"
     },
     "user_tz": -480
    },
    "id": "boEAxlB5uPZx",
    "outputId": "8886cc56-cb8e-41f0-ba52-d1387480790f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's draw some of the training data\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_94InEd1TkC6"
   },
   "source": [
    "# II. Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1730966517800,
     "user": {
      "displayName": "Hsin-En Tsai",
      "userId": "01830060851848219976"
     },
     "user_tz": -480
    },
    "id": "y_kwOzXQuWNK"
   },
   "outputs": [],
   "source": [
    "from os import X_OK\n",
    "\n",
    "# This class implements a minimal network (which still does okay)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Valid convolution, 1 channel in, 2 channels out, stride 1, kernel size = 3\n",
    "        self.conv1 = nn.Conv2d(1, 2, kernel_size=3)\n",
    "        # Dropout for convolutions\n",
    "        self.drop = nn.Dropout2d()\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(338, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.drop(x)\n",
    "        x = F.max_pool2d(x,2)\n",
    "        x = F.relu(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.log_softmax(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1730966517801,
     "user": {
      "displayName": "Hsin-En Tsai",
      "userId": "01830060851848219976"
     },
     "user_tz": -480
    },
    "id": "d4Ue45Pnf8gZ"
   },
   "outputs": [],
   "source": [
    "# TODO: Change above Net to Net2 class to implement\n",
    "\n",
    "# 1. A valid convolution with kernel size 5, 1 input channel and 10 output channels\n",
    "# 2. A max pooling operation over a 2x2 area\n",
    "# 3. A Relu\n",
    "# 4. A valid convolution with kernel size 5, 10 input channels and 20 output channels\n",
    "# 5. A 2D Dropout layer\n",
    "# 6. A max pooling operation over a 2x2 area\n",
    "# 7. A relu\n",
    "# 8. A flattening operation\n",
    "# 9. A fully connected layer mapping from (whatever dimensions we are at-- find out using .shape) to 50\n",
    "# 10. A ReLU\n",
    "# 11. A fully connected layer mapping from 50 to 10 dimensions\n",
    "# 12. A softmax function.\n",
    "\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(Net2, self).__init__()\n",
    "      # First convolutional layer: kernel size 5, 1 input channel, 10 output channels\n",
    "      self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "\n",
    "      # Second convolutional layer: kernel size 5, 10 input channels, 20 output channels\n",
    "      self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "\n",
    "      # 2D Dropout layer\n",
    "      self.drop = nn.Dropout2d()\n",
    "\n",
    "      # Initializing to 320 based on input size reduction\n",
    "      self.fc1 = nn.Linear(320, 50)\n",
    "\n",
    "      # Fully connected layer (50 to 10 dimensions)\n",
    "      self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "      # step 1: Convolution layer\n",
    "      x = self.conv1(x)\n",
    "\n",
    "      # step 2: Max pooling operation over 2x2 area\n",
    "      x = F.max_pool2d(x, 2)\n",
    "\n",
    "      # step 3: ReLU activation\n",
    "      x = F.relu(x)\n",
    "\n",
    "      # step 4: Second convolution layer\n",
    "      x = self.conv2(x)\n",
    "\n",
    "      # step 5: Dropout layer\n",
    "      x = self.drop(x)\n",
    "\n",
    "      # step 6: Max pooling operation over 2x2 area\n",
    "      x = F.max_pool2d(x, 2)\n",
    "\n",
    "      # step 7: ReLU activation\n",
    "      x = F.relu(x)\n",
    "\n",
    "      # step 8: Flattening operation\n",
    "      x = x.view(-1, 320)  # 320 here based on the output shape after convolution and pooling\n",
    "\n",
    "      # step 9: Fully connected layer\n",
    "      x = self.fc1(x)\n",
    "\n",
    "      # step 10: ReLU activation\n",
    "      x = F.relu(x)\n",
    "\n",
    "      # step 11: Fully connected layer\n",
    "      x = self.fc2(x)\n",
    "\n",
    "      # step 12: Softmax activation\n",
    "      x = F.log_softmax(x, dim=1)\n",
    "\n",
    "      return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1730966517801,
     "user": {
      "displayName": "Hsin-En Tsai",
      "userId": "01830060851848219976"
     },
     "user_tz": -480
    },
    "id": "9sN5hsK2uan8"
   },
   "outputs": [],
   "source": [
    "# He initialization of weights\n",
    "def weights_init(layer_in):\n",
    "  if isinstance(layer_in, nn.Linear):\n",
    "    nn.init.kaiming_uniform_(layer_in.weight)\n",
    "    layer_in.bias.data.fill_(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1730966517802,
     "user": {
      "displayName": "Hsin-En Tsai",
      "userId": "01830060851848219976"
     },
     "user_tz": -480
    },
    "id": "2pBDgYp2ufUi"
   },
   "outputs": [],
   "source": [
    "# Main training routine\n",
    "# TODO: Read it and understand what it does, you would need to implement it in the next colab HW\n",
    "\n",
    "# 每次結束時計算 loss，每隔10次 output 一次訓練狀態\n",
    "def train(epoch, model):\n",
    "  model.train()\n",
    "  # Get each\n",
    "  for batch_idx, (data, target) in enumerate(train_loader): # batch_idx 是批次index，從0開始\n",
    "    # (data, target): (影像數據, 對應標籤)\n",
    "    optimizer.zero_grad()  # 將上次計算的梯度歸零，防止累積梯度影響本次更新\n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output, target)  # negative log\n",
    "    loss.backward()  # 計算每個參數梯度\n",
    "    optimizer.step()  #　更新模型參數\n",
    "    # Store results\n",
    "    if batch_idx % 10 == 0:\n",
    "      pred = output.data.max(1, keepdim=True)[1]  # 找機率最大的位置（預測類別）\n",
    "      correct = pred.eq(target.data.view_as(pred)).sum()  # 比較預測pred和實際target，正確預測量\n",
    "      print('Train Epoch: {} [{}/{}]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset), loss.item()))\n",
    "      #  每10次輸出當前訓練狀態，包含訓練輪數、已訓練樣本數、總樣本數和當前損失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1730966517802,
     "user": {
      "displayName": "Hsin-En Tsai",
      "userId": "01830060851848219976"
     },
     "user_tz": -480
    },
    "id": "Xr6yXzWduhbU"
   },
   "outputs": [],
   "source": [
    "# Run on test data\n",
    "# TODO: Read it and understand what it does, you would need to implement it in the next colab HW\n",
    "\n",
    "def test(model):\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():  # 不用梯度計算以加速推理過程，並減少內存使用\n",
    "    for data, target in test_loader:\n",
    "      output = model(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()  # size_average:不取平均，累積損失值以便後續計算平均損失\n",
    "      pred = output.data.max(1, keepdim=True)[1]  # 機率最大的預測類別\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()  # 正確預測數量\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n",
    "  # output 測試結果，包括平均損失、正確預測樣本數、總樣本數和準確率\n",
    "  return 100. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 211375,
     "status": "ok",
     "timestamp": 1730966729167,
     "user": {
      "displayName": "Hsin-En Tsai",
      "userId": "01830060851848219976"
     },
     "user_tz": -480
    },
    "id": "EVUrbYiamki8",
    "outputId": "a4ac759d-c104-4d59-97c6-357dedef1a68"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-b8ebf8cbff0b>:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.log_softmax(x)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.4526, Accuracy: 1446/10000 (14%)\n",
      "\n",
      "Train Epoch: 1 [0/60000]\tLoss: 2.543816\n",
      "Train Epoch: 1 [640/60000]\tLoss: 2.430902\n",
      "Train Epoch: 1 [1280/60000]\tLoss: 2.184910\n",
      "Train Epoch: 1 [1920/60000]\tLoss: 2.131505\n",
      "Train Epoch: 1 [2560/60000]\tLoss: 1.966648\n",
      "Train Epoch: 1 [3200/60000]\tLoss: 1.814692\n",
      "Train Epoch: 1 [3840/60000]\tLoss: 1.578962\n",
      "Train Epoch: 1 [4480/60000]\tLoss: 1.937963\n",
      "Train Epoch: 1 [5120/60000]\tLoss: 1.588775\n",
      "Train Epoch: 1 [5760/60000]\tLoss: 1.628160\n",
      "Train Epoch: 1 [6400/60000]\tLoss: 1.499397\n",
      "Train Epoch: 1 [7040/60000]\tLoss: 1.496967\n",
      "Train Epoch: 1 [7680/60000]\tLoss: 1.679612\n",
      "Train Epoch: 1 [8320/60000]\tLoss: 1.224873\n",
      "Train Epoch: 1 [8960/60000]\tLoss: 1.311262\n",
      "Train Epoch: 1 [9600/60000]\tLoss: 1.249011\n",
      "Train Epoch: 1 [10240/60000]\tLoss: 1.203141\n",
      "Train Epoch: 1 [10880/60000]\tLoss: 1.131563\n",
      "Train Epoch: 1 [11520/60000]\tLoss: 1.173080\n",
      "Train Epoch: 1 [12160/60000]\tLoss: 1.120545\n",
      "Train Epoch: 1 [12800/60000]\tLoss: 1.263609\n",
      "Train Epoch: 1 [13440/60000]\tLoss: 1.251953\n",
      "Train Epoch: 1 [14080/60000]\tLoss: 0.955944\n",
      "Train Epoch: 1 [14720/60000]\tLoss: 1.192998\n",
      "Train Epoch: 1 [15360/60000]\tLoss: 1.169931\n",
      "Train Epoch: 1 [16000/60000]\tLoss: 1.375689\n",
      "Train Epoch: 1 [16640/60000]\tLoss: 1.075608\n",
      "Train Epoch: 1 [17280/60000]\tLoss: 1.043292\n",
      "Train Epoch: 1 [17920/60000]\tLoss: 1.060902\n",
      "Train Epoch: 1 [18560/60000]\tLoss: 1.030562\n",
      "Train Epoch: 1 [19200/60000]\tLoss: 1.149238\n",
      "Train Epoch: 1 [19840/60000]\tLoss: 0.894466\n",
      "Train Epoch: 1 [20480/60000]\tLoss: 0.962834\n",
      "Train Epoch: 1 [21120/60000]\tLoss: 0.845940\n",
      "Train Epoch: 1 [21760/60000]\tLoss: 0.947014\n",
      "Train Epoch: 1 [22400/60000]\tLoss: 0.822830\n",
      "Train Epoch: 1 [23040/60000]\tLoss: 1.060214\n",
      "Train Epoch: 1 [23680/60000]\tLoss: 1.147049\n",
      "Train Epoch: 1 [24320/60000]\tLoss: 1.085690\n",
      "Train Epoch: 1 [24960/60000]\tLoss: 0.997768\n",
      "Train Epoch: 1 [25600/60000]\tLoss: 1.081243\n",
      "Train Epoch: 1 [26240/60000]\tLoss: 0.839314\n",
      "Train Epoch: 1 [26880/60000]\tLoss: 1.039657\n",
      "Train Epoch: 1 [27520/60000]\tLoss: 0.944455\n",
      "Train Epoch: 1 [28160/60000]\tLoss: 1.283222\n",
      "Train Epoch: 1 [28800/60000]\tLoss: 0.941479\n",
      "Train Epoch: 1 [29440/60000]\tLoss: 0.971420\n",
      "Train Epoch: 1 [30080/60000]\tLoss: 0.653577\n",
      "Train Epoch: 1 [30720/60000]\tLoss: 0.743445\n",
      "Train Epoch: 1 [31360/60000]\tLoss: 0.743523\n",
      "Train Epoch: 1 [32000/60000]\tLoss: 1.215317\n",
      "Train Epoch: 1 [32640/60000]\tLoss: 0.823124\n",
      "Train Epoch: 1 [33280/60000]\tLoss: 0.739985\n",
      "Train Epoch: 1 [33920/60000]\tLoss: 1.205007\n",
      "Train Epoch: 1 [34560/60000]\tLoss: 0.868437\n",
      "Train Epoch: 1 [35200/60000]\tLoss: 0.985446\n",
      "Train Epoch: 1 [35840/60000]\tLoss: 1.130209\n",
      "Train Epoch: 1 [36480/60000]\tLoss: 1.126982\n",
      "Train Epoch: 1 [37120/60000]\tLoss: 0.743489\n",
      "Train Epoch: 1 [37760/60000]\tLoss: 1.000694\n",
      "Train Epoch: 1 [38400/60000]\tLoss: 0.948607\n",
      "Train Epoch: 1 [39040/60000]\tLoss: 1.095634\n",
      "Train Epoch: 1 [39680/60000]\tLoss: 0.603563\n",
      "Train Epoch: 1 [40320/60000]\tLoss: 0.925311\n",
      "Train Epoch: 1 [40960/60000]\tLoss: 0.894607\n",
      "Train Epoch: 1 [41600/60000]\tLoss: 0.812576\n",
      "Train Epoch: 1 [42240/60000]\tLoss: 0.857148\n",
      "Train Epoch: 1 [42880/60000]\tLoss: 0.932841\n",
      "Train Epoch: 1 [43520/60000]\tLoss: 0.989617\n",
      "Train Epoch: 1 [44160/60000]\tLoss: 0.824893\n",
      "Train Epoch: 1 [44800/60000]\tLoss: 0.797720\n",
      "Train Epoch: 1 [45440/60000]\tLoss: 0.891083\n",
      "Train Epoch: 1 [46080/60000]\tLoss: 1.110471\n",
      "Train Epoch: 1 [46720/60000]\tLoss: 1.177714\n",
      "Train Epoch: 1 [47360/60000]\tLoss: 0.602137\n",
      "Train Epoch: 1 [48000/60000]\tLoss: 0.895038\n",
      "Train Epoch: 1 [48640/60000]\tLoss: 0.874484\n",
      "Train Epoch: 1 [49280/60000]\tLoss: 0.668745\n",
      "Train Epoch: 1 [49920/60000]\tLoss: 1.263860\n",
      "Train Epoch: 1 [50560/60000]\tLoss: 0.973751\n",
      "Train Epoch: 1 [51200/60000]\tLoss: 0.765533\n",
      "Train Epoch: 1 [51840/60000]\tLoss: 0.766048\n",
      "Train Epoch: 1 [52480/60000]\tLoss: 0.912812\n",
      "Train Epoch: 1 [53120/60000]\tLoss: 0.755511\n",
      "Train Epoch: 1 [53760/60000]\tLoss: 1.102347\n",
      "Train Epoch: 1 [54400/60000]\tLoss: 1.079788\n",
      "Train Epoch: 1 [55040/60000]\tLoss: 1.038113\n",
      "Train Epoch: 1 [55680/60000]\tLoss: 0.853759\n",
      "Train Epoch: 1 [56320/60000]\tLoss: 1.118938\n",
      "Train Epoch: 1 [56960/60000]\tLoss: 1.038675\n",
      "Train Epoch: 1 [57600/60000]\tLoss: 1.313032\n",
      "Train Epoch: 1 [58240/60000]\tLoss: 0.752628\n",
      "Train Epoch: 1 [58880/60000]\tLoss: 1.182663\n",
      "Train Epoch: 1 [59520/60000]\tLoss: 0.928226\n",
      "Train Epoch: 2 [0/60000]\tLoss: 0.865871\n",
      "Train Epoch: 2 [640/60000]\tLoss: 0.915005\n",
      "Train Epoch: 2 [1280/60000]\tLoss: 0.862644\n",
      "Train Epoch: 2 [1920/60000]\tLoss: 1.017747\n",
      "Train Epoch: 2 [2560/60000]\tLoss: 1.030814\n",
      "Train Epoch: 2 [3200/60000]\tLoss: 0.785639\n",
      "Train Epoch: 2 [3840/60000]\tLoss: 0.951402\n",
      "Train Epoch: 2 [4480/60000]\tLoss: 1.234114\n",
      "Train Epoch: 2 [5120/60000]\tLoss: 0.954800\n",
      "Train Epoch: 2 [5760/60000]\tLoss: 0.712744\n",
      "Train Epoch: 2 [6400/60000]\tLoss: 1.237276\n",
      "Train Epoch: 2 [7040/60000]\tLoss: 0.933963\n",
      "Train Epoch: 2 [7680/60000]\tLoss: 0.746896\n",
      "Train Epoch: 2 [8320/60000]\tLoss: 1.096500\n",
      "Train Epoch: 2 [8960/60000]\tLoss: 0.965420\n",
      "Train Epoch: 2 [9600/60000]\tLoss: 0.788782\n",
      "Train Epoch: 2 [10240/60000]\tLoss: 1.165296\n",
      "Train Epoch: 2 [10880/60000]\tLoss: 0.844595\n",
      "Train Epoch: 2 [11520/60000]\tLoss: 0.812368\n",
      "Train Epoch: 2 [12160/60000]\tLoss: 0.692152\n",
      "Train Epoch: 2 [12800/60000]\tLoss: 0.980859\n",
      "Train Epoch: 2 [13440/60000]\tLoss: 0.630005\n",
      "Train Epoch: 2 [14080/60000]\tLoss: 0.838174\n",
      "Train Epoch: 2 [14720/60000]\tLoss: 1.150043\n",
      "Train Epoch: 2 [15360/60000]\tLoss: 1.034172\n",
      "Train Epoch: 2 [16000/60000]\tLoss: 0.751046\n",
      "Train Epoch: 2 [16640/60000]\tLoss: 0.535891\n",
      "Train Epoch: 2 [17280/60000]\tLoss: 0.709253\n",
      "Train Epoch: 2 [17920/60000]\tLoss: 1.260291\n",
      "Train Epoch: 2 [18560/60000]\tLoss: 0.851238\n",
      "Train Epoch: 2 [19200/60000]\tLoss: 0.837803\n",
      "Train Epoch: 2 [19840/60000]\tLoss: 1.056309\n",
      "Train Epoch: 2 [20480/60000]\tLoss: 1.070083\n",
      "Train Epoch: 2 [21120/60000]\tLoss: 0.909098\n",
      "Train Epoch: 2 [21760/60000]\tLoss: 0.476137\n",
      "Train Epoch: 2 [22400/60000]\tLoss: 1.002066\n",
      "Train Epoch: 2 [23040/60000]\tLoss: 1.009195\n",
      "Train Epoch: 2 [23680/60000]\tLoss: 0.842547\n",
      "Train Epoch: 2 [24320/60000]\tLoss: 1.042441\n",
      "Train Epoch: 2 [24960/60000]\tLoss: 0.818663\n",
      "Train Epoch: 2 [25600/60000]\tLoss: 0.798837\n",
      "Train Epoch: 2 [26240/60000]\tLoss: 0.828135\n",
      "Train Epoch: 2 [26880/60000]\tLoss: 0.675536\n",
      "Train Epoch: 2 [27520/60000]\tLoss: 0.908663\n",
      "Train Epoch: 2 [28160/60000]\tLoss: 0.679748\n",
      "Train Epoch: 2 [28800/60000]\tLoss: 0.863829\n",
      "Train Epoch: 2 [29440/60000]\tLoss: 0.798246\n",
      "Train Epoch: 2 [30080/60000]\tLoss: 1.074093\n",
      "Train Epoch: 2 [30720/60000]\tLoss: 1.059092\n",
      "Train Epoch: 2 [31360/60000]\tLoss: 0.675135\n",
      "Train Epoch: 2 [32000/60000]\tLoss: 0.891973\n",
      "Train Epoch: 2 [32640/60000]\tLoss: 1.084372\n",
      "Train Epoch: 2 [33280/60000]\tLoss: 1.143370\n",
      "Train Epoch: 2 [33920/60000]\tLoss: 0.698762\n",
      "Train Epoch: 2 [34560/60000]\tLoss: 0.864072\n",
      "Train Epoch: 2 [35200/60000]\tLoss: 0.990068\n",
      "Train Epoch: 2 [35840/60000]\tLoss: 1.141230\n",
      "Train Epoch: 2 [36480/60000]\tLoss: 0.626342\n",
      "Train Epoch: 2 [37120/60000]\tLoss: 1.262307\n",
      "Train Epoch: 2 [37760/60000]\tLoss: 1.104483\n",
      "Train Epoch: 2 [38400/60000]\tLoss: 0.841109\n",
      "Train Epoch: 2 [39040/60000]\tLoss: 0.836421\n",
      "Train Epoch: 2 [39680/60000]\tLoss: 0.847700\n",
      "Train Epoch: 2 [40320/60000]\tLoss: 0.887178\n",
      "Train Epoch: 2 [40960/60000]\tLoss: 0.899502\n",
      "Train Epoch: 2 [41600/60000]\tLoss: 0.918329\n",
      "Train Epoch: 2 [42240/60000]\tLoss: 0.951792\n",
      "Train Epoch: 2 [42880/60000]\tLoss: 0.959292\n",
      "Train Epoch: 2 [43520/60000]\tLoss: 0.604309\n",
      "Train Epoch: 2 [44160/60000]\tLoss: 0.887072\n",
      "Train Epoch: 2 [44800/60000]\tLoss: 0.978999\n",
      "Train Epoch: 2 [45440/60000]\tLoss: 0.640377\n",
      "Train Epoch: 2 [46080/60000]\tLoss: 0.847304\n",
      "Train Epoch: 2 [46720/60000]\tLoss: 0.821488\n",
      "Train Epoch: 2 [47360/60000]\tLoss: 1.248274\n",
      "Train Epoch: 2 [48000/60000]\tLoss: 0.935493\n",
      "Train Epoch: 2 [48640/60000]\tLoss: 1.149518\n",
      "Train Epoch: 2 [49280/60000]\tLoss: 0.800794\n",
      "Train Epoch: 2 [49920/60000]\tLoss: 0.624437\n",
      "Train Epoch: 2 [50560/60000]\tLoss: 1.045192\n",
      "Train Epoch: 2 [51200/60000]\tLoss: 0.786415\n",
      "Train Epoch: 2 [51840/60000]\tLoss: 0.968931\n",
      "Train Epoch: 2 [52480/60000]\tLoss: 0.921235\n",
      "Train Epoch: 2 [53120/60000]\tLoss: 1.008819\n",
      "Train Epoch: 2 [53760/60000]\tLoss: 1.024612\n",
      "Train Epoch: 2 [54400/60000]\tLoss: 0.805040\n",
      "Train Epoch: 2 [55040/60000]\tLoss: 0.773435\n",
      "Train Epoch: 2 [55680/60000]\tLoss: 0.818308\n",
      "Train Epoch: 2 [56320/60000]\tLoss: 0.898803\n",
      "Train Epoch: 2 [56960/60000]\tLoss: 0.871045\n",
      "Train Epoch: 2 [57600/60000]\tLoss: 0.897580\n",
      "Train Epoch: 2 [58240/60000]\tLoss: 0.909725\n",
      "Train Epoch: 2 [58880/60000]\tLoss: 1.037531\n",
      "Train Epoch: 2 [59520/60000]\tLoss: 0.926117\n",
      "Train Epoch: 3 [0/60000]\tLoss: 1.016840\n",
      "Train Epoch: 3 [640/60000]\tLoss: 0.746784\n",
      "Train Epoch: 3 [1280/60000]\tLoss: 0.581786\n",
      "Train Epoch: 3 [1920/60000]\tLoss: 0.637210\n",
      "Train Epoch: 3 [2560/60000]\tLoss: 0.893411\n",
      "Train Epoch: 3 [3200/60000]\tLoss: 1.047512\n",
      "Train Epoch: 3 [3840/60000]\tLoss: 1.040843\n",
      "Train Epoch: 3 [4480/60000]\tLoss: 0.761454\n",
      "Train Epoch: 3 [5120/60000]\tLoss: 0.987738\n",
      "Train Epoch: 3 [5760/60000]\tLoss: 0.866519\n",
      "Train Epoch: 3 [6400/60000]\tLoss: 0.715002\n",
      "Train Epoch: 3 [7040/60000]\tLoss: 0.506670\n",
      "Train Epoch: 3 [7680/60000]\tLoss: 1.364158\n",
      "Train Epoch: 3 [8320/60000]\tLoss: 0.689170\n",
      "Train Epoch: 3 [8960/60000]\tLoss: 0.800220\n",
      "Train Epoch: 3 [9600/60000]\tLoss: 0.832638\n",
      "Train Epoch: 3 [10240/60000]\tLoss: 0.813331\n",
      "Train Epoch: 3 [10880/60000]\tLoss: 1.067847\n",
      "Train Epoch: 3 [11520/60000]\tLoss: 1.108344\n",
      "Train Epoch: 3 [12160/60000]\tLoss: 0.837182\n",
      "Train Epoch: 3 [12800/60000]\tLoss: 0.706780\n",
      "Train Epoch: 3 [13440/60000]\tLoss: 0.896841\n",
      "Train Epoch: 3 [14080/60000]\tLoss: 0.714613\n",
      "Train Epoch: 3 [14720/60000]\tLoss: 1.029459\n",
      "Train Epoch: 3 [15360/60000]\tLoss: 1.188834\n",
      "Train Epoch: 3 [16000/60000]\tLoss: 0.853845\n",
      "Train Epoch: 3 [16640/60000]\tLoss: 0.707504\n",
      "Train Epoch: 3 [17280/60000]\tLoss: 1.007409\n",
      "Train Epoch: 3 [17920/60000]\tLoss: 0.557201\n",
      "Train Epoch: 3 [18560/60000]\tLoss: 1.210795\n",
      "Train Epoch: 3 [19200/60000]\tLoss: 0.619659\n",
      "Train Epoch: 3 [19840/60000]\tLoss: 0.847009\n",
      "Train Epoch: 3 [20480/60000]\tLoss: 0.859970\n",
      "Train Epoch: 3 [21120/60000]\tLoss: 0.941030\n",
      "Train Epoch: 3 [21760/60000]\tLoss: 0.790008\n",
      "Train Epoch: 3 [22400/60000]\tLoss: 0.985204\n",
      "Train Epoch: 3 [23040/60000]\tLoss: 0.870283\n",
      "Train Epoch: 3 [23680/60000]\tLoss: 0.736105\n",
      "Train Epoch: 3 [24320/60000]\tLoss: 0.966622\n",
      "Train Epoch: 3 [24960/60000]\tLoss: 0.700487\n",
      "Train Epoch: 3 [25600/60000]\tLoss: 0.598810\n",
      "Train Epoch: 3 [26240/60000]\tLoss: 0.988467\n",
      "Train Epoch: 3 [26880/60000]\tLoss: 1.257616\n",
      "Train Epoch: 3 [27520/60000]\tLoss: 0.929830\n",
      "Train Epoch: 3 [28160/60000]\tLoss: 0.705288\n",
      "Train Epoch: 3 [28800/60000]\tLoss: 1.069960\n",
      "Train Epoch: 3 [29440/60000]\tLoss: 0.934583\n",
      "Train Epoch: 3 [30080/60000]\tLoss: 0.519940\n",
      "Train Epoch: 3 [30720/60000]\tLoss: 0.962165\n",
      "Train Epoch: 3 [31360/60000]\tLoss: 1.179438\n",
      "Train Epoch: 3 [32000/60000]\tLoss: 0.783110\n",
      "Train Epoch: 3 [32640/60000]\tLoss: 0.858951\n",
      "Train Epoch: 3 [33280/60000]\tLoss: 0.791296\n",
      "Train Epoch: 3 [33920/60000]\tLoss: 0.921881\n",
      "Train Epoch: 3 [34560/60000]\tLoss: 0.859515\n",
      "Train Epoch: 3 [35200/60000]\tLoss: 0.939806\n",
      "Train Epoch: 3 [35840/60000]\tLoss: 0.693514\n",
      "Train Epoch: 3 [36480/60000]\tLoss: 0.699315\n",
      "Train Epoch: 3 [37120/60000]\tLoss: 0.611382\n",
      "Train Epoch: 3 [37760/60000]\tLoss: 0.938241\n",
      "Train Epoch: 3 [38400/60000]\tLoss: 0.535412\n",
      "Train Epoch: 3 [39040/60000]\tLoss: 0.661435\n",
      "Train Epoch: 3 [39680/60000]\tLoss: 1.078123\n",
      "Train Epoch: 3 [40320/60000]\tLoss: 0.857115\n",
      "Train Epoch: 3 [40960/60000]\tLoss: 0.617021\n",
      "Train Epoch: 3 [41600/60000]\tLoss: 0.847423\n",
      "Train Epoch: 3 [42240/60000]\tLoss: 0.962388\n",
      "Train Epoch: 3 [42880/60000]\tLoss: 0.552855\n",
      "Train Epoch: 3 [43520/60000]\tLoss: 0.821119\n",
      "Train Epoch: 3 [44160/60000]\tLoss: 0.912280\n",
      "Train Epoch: 3 [44800/60000]\tLoss: 1.037364\n",
      "Train Epoch: 3 [45440/60000]\tLoss: 0.848117\n",
      "Train Epoch: 3 [46080/60000]\tLoss: 0.841048\n",
      "Train Epoch: 3 [46720/60000]\tLoss: 1.199317\n",
      "Train Epoch: 3 [47360/60000]\tLoss: 0.957097\n",
      "Train Epoch: 3 [48000/60000]\tLoss: 0.898710\n",
      "Train Epoch: 3 [48640/60000]\tLoss: 0.716366\n",
      "Train Epoch: 3 [49280/60000]\tLoss: 0.826268\n",
      "Train Epoch: 3 [49920/60000]\tLoss: 1.128554\n",
      "Train Epoch: 3 [50560/60000]\tLoss: 0.862854\n",
      "Train Epoch: 3 [51200/60000]\tLoss: 0.920273\n",
      "Train Epoch: 3 [51840/60000]\tLoss: 0.788986\n",
      "Train Epoch: 3 [52480/60000]\tLoss: 0.941375\n",
      "Train Epoch: 3 [53120/60000]\tLoss: 0.663078\n",
      "Train Epoch: 3 [53760/60000]\tLoss: 0.923269\n",
      "Train Epoch: 3 [54400/60000]\tLoss: 0.679529\n",
      "Train Epoch: 3 [55040/60000]\tLoss: 0.910848\n",
      "Train Epoch: 3 [55680/60000]\tLoss: 1.021707\n",
      "Train Epoch: 3 [56320/60000]\tLoss: 0.916582\n",
      "Train Epoch: 3 [56960/60000]\tLoss: 0.832631\n",
      "Train Epoch: 3 [57600/60000]\tLoss: 0.921901\n",
      "Train Epoch: 3 [58240/60000]\tLoss: 0.702057\n",
      "Train Epoch: 3 [58880/60000]\tLoss: 0.702576\n",
      "Train Epoch: 3 [59520/60000]\tLoss: 0.734939\n",
      "Train Epoch: 4 [0/60000]\tLoss: 1.104147\n",
      "Train Epoch: 4 [640/60000]\tLoss: 0.917127\n",
      "Train Epoch: 4 [1280/60000]\tLoss: 0.821754\n",
      "Train Epoch: 4 [1920/60000]\tLoss: 0.980570\n",
      "Train Epoch: 4 [2560/60000]\tLoss: 0.598514\n",
      "Train Epoch: 4 [3200/60000]\tLoss: 0.786492\n",
      "Train Epoch: 4 [3840/60000]\tLoss: 0.704020\n",
      "Train Epoch: 4 [4480/60000]\tLoss: 0.793300\n",
      "Train Epoch: 4 [5120/60000]\tLoss: 0.379636\n",
      "Train Epoch: 4 [5760/60000]\tLoss: 0.800826\n",
      "Train Epoch: 4 [6400/60000]\tLoss: 0.678151\n",
      "Train Epoch: 4 [7040/60000]\tLoss: 0.996793\n",
      "Train Epoch: 4 [7680/60000]\tLoss: 0.759111\n",
      "Train Epoch: 4 [8320/60000]\tLoss: 0.766901\n",
      "Train Epoch: 4 [8960/60000]\tLoss: 0.768890\n",
      "Train Epoch: 4 [9600/60000]\tLoss: 0.993950\n",
      "Train Epoch: 4 [10240/60000]\tLoss: 0.885980\n",
      "Train Epoch: 4 [10880/60000]\tLoss: 0.877157\n",
      "Train Epoch: 4 [11520/60000]\tLoss: 0.952927\n",
      "Train Epoch: 4 [12160/60000]\tLoss: 0.778380\n",
      "Train Epoch: 4 [12800/60000]\tLoss: 0.984403\n",
      "Train Epoch: 4 [13440/60000]\tLoss: 0.819403\n",
      "Train Epoch: 4 [14080/60000]\tLoss: 0.641858\n",
      "Train Epoch: 4 [14720/60000]\tLoss: 0.929794\n",
      "Train Epoch: 4 [15360/60000]\tLoss: 1.034021\n",
      "Train Epoch: 4 [16000/60000]\tLoss: 0.530822\n",
      "Train Epoch: 4 [16640/60000]\tLoss: 0.912659\n",
      "Train Epoch: 4 [17280/60000]\tLoss: 0.750383\n",
      "Train Epoch: 4 [17920/60000]\tLoss: 0.680103\n",
      "Train Epoch: 4 [18560/60000]\tLoss: 1.242671\n",
      "Train Epoch: 4 [19200/60000]\tLoss: 0.879523\n",
      "Train Epoch: 4 [19840/60000]\tLoss: 0.757748\n",
      "Train Epoch: 4 [20480/60000]\tLoss: 0.970815\n",
      "Train Epoch: 4 [21120/60000]\tLoss: 0.680692\n",
      "Train Epoch: 4 [21760/60000]\tLoss: 0.977956\n",
      "Train Epoch: 4 [22400/60000]\tLoss: 0.934148\n",
      "Train Epoch: 4 [23040/60000]\tLoss: 0.822366\n",
      "Train Epoch: 4 [23680/60000]\tLoss: 0.853785\n",
      "Train Epoch: 4 [24320/60000]\tLoss: 0.909620\n",
      "Train Epoch: 4 [24960/60000]\tLoss: 0.711738\n",
      "Train Epoch: 4 [25600/60000]\tLoss: 0.589941\n",
      "Train Epoch: 4 [26240/60000]\tLoss: 0.842290\n",
      "Train Epoch: 4 [26880/60000]\tLoss: 1.118408\n",
      "Train Epoch: 4 [27520/60000]\tLoss: 0.619497\n",
      "Train Epoch: 4 [28160/60000]\tLoss: 1.122993\n",
      "Train Epoch: 4 [28800/60000]\tLoss: 0.782985\n",
      "Train Epoch: 4 [29440/60000]\tLoss: 0.822540\n",
      "Train Epoch: 4 [30080/60000]\tLoss: 1.153845\n",
      "Train Epoch: 4 [30720/60000]\tLoss: 1.051624\n",
      "Train Epoch: 4 [31360/60000]\tLoss: 1.013306\n",
      "Train Epoch: 4 [32000/60000]\tLoss: 0.972333\n",
      "Train Epoch: 4 [32640/60000]\tLoss: 0.924071\n",
      "Train Epoch: 4 [33280/60000]\tLoss: 0.626724\n",
      "Train Epoch: 4 [33920/60000]\tLoss: 0.927318\n",
      "Train Epoch: 4 [34560/60000]\tLoss: 0.563400\n",
      "Train Epoch: 4 [35200/60000]\tLoss: 0.866411\n",
      "Train Epoch: 4 [35840/60000]\tLoss: 0.843939\n",
      "Train Epoch: 4 [36480/60000]\tLoss: 0.756544\n",
      "Train Epoch: 4 [37120/60000]\tLoss: 1.199251\n",
      "Train Epoch: 4 [37760/60000]\tLoss: 0.772030\n",
      "Train Epoch: 4 [38400/60000]\tLoss: 0.866837\n",
      "Train Epoch: 4 [39040/60000]\tLoss: 0.958821\n",
      "Train Epoch: 4 [39680/60000]\tLoss: 0.828577\n",
      "Train Epoch: 4 [40320/60000]\tLoss: 0.993745\n",
      "Train Epoch: 4 [40960/60000]\tLoss: 0.800923\n",
      "Train Epoch: 4 [41600/60000]\tLoss: 1.024539\n",
      "Train Epoch: 4 [42240/60000]\tLoss: 0.964469\n",
      "Train Epoch: 4 [42880/60000]\tLoss: 1.010631\n",
      "Train Epoch: 4 [43520/60000]\tLoss: 0.990658\n",
      "Train Epoch: 4 [44160/60000]\tLoss: 1.256113\n",
      "Train Epoch: 4 [44800/60000]\tLoss: 0.837782\n",
      "Train Epoch: 4 [45440/60000]\tLoss: 0.853322\n",
      "Train Epoch: 4 [46080/60000]\tLoss: 0.740621\n",
      "Train Epoch: 4 [46720/60000]\tLoss: 0.693636\n",
      "Train Epoch: 4 [47360/60000]\tLoss: 0.891561\n",
      "Train Epoch: 4 [48000/60000]\tLoss: 0.888777\n",
      "Train Epoch: 4 [48640/60000]\tLoss: 0.733899\n",
      "Train Epoch: 4 [49280/60000]\tLoss: 0.905213\n",
      "Train Epoch: 4 [49920/60000]\tLoss: 1.005558\n",
      "Train Epoch: 4 [50560/60000]\tLoss: 0.949108\n",
      "Train Epoch: 4 [51200/60000]\tLoss: 0.935845\n",
      "Train Epoch: 4 [51840/60000]\tLoss: 0.699897\n",
      "Train Epoch: 4 [52480/60000]\tLoss: 0.890202\n",
      "Train Epoch: 4 [53120/60000]\tLoss: 0.690776\n",
      "Train Epoch: 4 [53760/60000]\tLoss: 0.865538\n",
      "Train Epoch: 4 [54400/60000]\tLoss: 0.568816\n",
      "Train Epoch: 4 [55040/60000]\tLoss: 0.909102\n",
      "Train Epoch: 4 [55680/60000]\tLoss: 0.570453\n",
      "Train Epoch: 4 [56320/60000]\tLoss: 0.781486\n",
      "Train Epoch: 4 [56960/60000]\tLoss: 0.805801\n",
      "Train Epoch: 4 [57600/60000]\tLoss: 0.753800\n",
      "Train Epoch: 4 [58240/60000]\tLoss: 0.583306\n",
      "Train Epoch: 4 [58880/60000]\tLoss: 0.554984\n",
      "Train Epoch: 4 [59520/60000]\tLoss: 0.799056\n",
      "Train Epoch: 5 [0/60000]\tLoss: 0.769773\n",
      "Train Epoch: 5 [640/60000]\tLoss: 0.732678\n",
      "Train Epoch: 5 [1280/60000]\tLoss: 0.621850\n",
      "Train Epoch: 5 [1920/60000]\tLoss: 0.704973\n",
      "Train Epoch: 5 [2560/60000]\tLoss: 0.688452\n",
      "Train Epoch: 5 [3200/60000]\tLoss: 0.811277\n",
      "Train Epoch: 5 [3840/60000]\tLoss: 1.185156\n",
      "Train Epoch: 5 [4480/60000]\tLoss: 0.644265\n",
      "Train Epoch: 5 [5120/60000]\tLoss: 0.624795\n",
      "Train Epoch: 5 [5760/60000]\tLoss: 0.893929\n",
      "Train Epoch: 5 [6400/60000]\tLoss: 0.953004\n",
      "Train Epoch: 5 [7040/60000]\tLoss: 1.031446\n",
      "Train Epoch: 5 [7680/60000]\tLoss: 1.061080\n",
      "Train Epoch: 5 [8320/60000]\tLoss: 0.637140\n",
      "Train Epoch: 5 [8960/60000]\tLoss: 0.664633\n",
      "Train Epoch: 5 [9600/60000]\tLoss: 0.833603\n",
      "Train Epoch: 5 [10240/60000]\tLoss: 0.826386\n",
      "Train Epoch: 5 [10880/60000]\tLoss: 1.046294\n",
      "Train Epoch: 5 [11520/60000]\tLoss: 0.688007\n",
      "Train Epoch: 5 [12160/60000]\tLoss: 0.833347\n",
      "Train Epoch: 5 [12800/60000]\tLoss: 0.831278\n",
      "Train Epoch: 5 [13440/60000]\tLoss: 0.915717\n",
      "Train Epoch: 5 [14080/60000]\tLoss: 0.857873\n",
      "Train Epoch: 5 [14720/60000]\tLoss: 0.875492\n",
      "Train Epoch: 5 [15360/60000]\tLoss: 1.016835\n",
      "Train Epoch: 5 [16000/60000]\tLoss: 0.789517\n",
      "Train Epoch: 5 [16640/60000]\tLoss: 0.670681\n",
      "Train Epoch: 5 [17280/60000]\tLoss: 0.584367\n",
      "Train Epoch: 5 [17920/60000]\tLoss: 0.875652\n",
      "Train Epoch: 5 [18560/60000]\tLoss: 0.943268\n",
      "Train Epoch: 5 [19200/60000]\tLoss: 0.756733\n",
      "Train Epoch: 5 [19840/60000]\tLoss: 0.638722\n",
      "Train Epoch: 5 [20480/60000]\tLoss: 0.765320\n",
      "Train Epoch: 5 [21120/60000]\tLoss: 0.939883\n",
      "Train Epoch: 5 [21760/60000]\tLoss: 0.891761\n",
      "Train Epoch: 5 [22400/60000]\tLoss: 1.138595\n",
      "Train Epoch: 5 [23040/60000]\tLoss: 0.782199\n",
      "Train Epoch: 5 [23680/60000]\tLoss: 0.527829\n",
      "Train Epoch: 5 [24320/60000]\tLoss: 1.006636\n",
      "Train Epoch: 5 [24960/60000]\tLoss: 0.939021\n",
      "Train Epoch: 5 [25600/60000]\tLoss: 0.723951\n",
      "Train Epoch: 5 [26240/60000]\tLoss: 1.171812\n",
      "Train Epoch: 5 [26880/60000]\tLoss: 0.921240\n",
      "Train Epoch: 5 [27520/60000]\tLoss: 0.727278\n",
      "Train Epoch: 5 [28160/60000]\tLoss: 0.755157\n",
      "Train Epoch: 5 [28800/60000]\tLoss: 1.055703\n",
      "Train Epoch: 5 [29440/60000]\tLoss: 0.854581\n",
      "Train Epoch: 5 [30080/60000]\tLoss: 0.872877\n",
      "Train Epoch: 5 [30720/60000]\tLoss: 1.116949\n",
      "Train Epoch: 5 [31360/60000]\tLoss: 0.847890\n",
      "Train Epoch: 5 [32000/60000]\tLoss: 0.629469\n",
      "Train Epoch: 5 [32640/60000]\tLoss: 0.953893\n",
      "Train Epoch: 5 [33280/60000]\tLoss: 0.838643\n",
      "Train Epoch: 5 [33920/60000]\tLoss: 0.892558\n",
      "Train Epoch: 5 [34560/60000]\tLoss: 0.835214\n",
      "Train Epoch: 5 [35200/60000]\tLoss: 0.924314\n",
      "Train Epoch: 5 [35840/60000]\tLoss: 0.749911\n",
      "Train Epoch: 5 [36480/60000]\tLoss: 0.728045\n",
      "Train Epoch: 5 [37120/60000]\tLoss: 0.925479\n",
      "Train Epoch: 5 [37760/60000]\tLoss: 0.875318\n",
      "Train Epoch: 5 [38400/60000]\tLoss: 0.876365\n",
      "Train Epoch: 5 [39040/60000]\tLoss: 1.037361\n",
      "Train Epoch: 5 [39680/60000]\tLoss: 0.815280\n",
      "Train Epoch: 5 [40320/60000]\tLoss: 0.785036\n",
      "Train Epoch: 5 [40960/60000]\tLoss: 0.819758\n",
      "Train Epoch: 5 [41600/60000]\tLoss: 0.920840\n",
      "Train Epoch: 5 [42240/60000]\tLoss: 0.719687\n",
      "Train Epoch: 5 [42880/60000]\tLoss: 1.140099\n",
      "Train Epoch: 5 [43520/60000]\tLoss: 0.803696\n",
      "Train Epoch: 5 [44160/60000]\tLoss: 0.799393\n",
      "Train Epoch: 5 [44800/60000]\tLoss: 0.705204\n",
      "Train Epoch: 5 [45440/60000]\tLoss: 0.919087\n",
      "Train Epoch: 5 [46080/60000]\tLoss: 0.708038\n",
      "Train Epoch: 5 [46720/60000]\tLoss: 0.537090\n",
      "Train Epoch: 5 [47360/60000]\tLoss: 1.009777\n",
      "Train Epoch: 5 [48000/60000]\tLoss: 0.869867\n",
      "Train Epoch: 5 [48640/60000]\tLoss: 0.775019\n",
      "Train Epoch: 5 [49280/60000]\tLoss: 0.832869\n",
      "Train Epoch: 5 [49920/60000]\tLoss: 1.255710\n",
      "Train Epoch: 5 [50560/60000]\tLoss: 0.765486\n",
      "Train Epoch: 5 [51200/60000]\tLoss: 0.713340\n",
      "Train Epoch: 5 [51840/60000]\tLoss: 0.543446\n",
      "Train Epoch: 5 [52480/60000]\tLoss: 1.014374\n",
      "Train Epoch: 5 [53120/60000]\tLoss: 1.147970\n",
      "Train Epoch: 5 [53760/60000]\tLoss: 0.983500\n",
      "Train Epoch: 5 [54400/60000]\tLoss: 0.735643\n",
      "Train Epoch: 5 [55040/60000]\tLoss: 0.717566\n",
      "Train Epoch: 5 [55680/60000]\tLoss: 0.804051\n",
      "Train Epoch: 5 [56320/60000]\tLoss: 0.958339\n",
      "Train Epoch: 5 [56960/60000]\tLoss: 0.852533\n",
      "Train Epoch: 5 [57600/60000]\tLoss: 0.555342\n",
      "Train Epoch: 5 [58240/60000]\tLoss: 0.723641\n",
      "Train Epoch: 5 [58880/60000]\tLoss: 0.768648\n",
      "Train Epoch: 5 [59520/60000]\tLoss: 0.892386\n",
      "Train Epoch: 6 [0/60000]\tLoss: 0.823086\n",
      "Train Epoch: 6 [640/60000]\tLoss: 1.005974\n",
      "Train Epoch: 6 [1280/60000]\tLoss: 0.840996\n",
      "Train Epoch: 6 [1920/60000]\tLoss: 0.839154\n",
      "Train Epoch: 6 [2560/60000]\tLoss: 0.665415\n",
      "Train Epoch: 6 [3200/60000]\tLoss: 0.967441\n",
      "Train Epoch: 6 [3840/60000]\tLoss: 0.810062\n",
      "Train Epoch: 6 [4480/60000]\tLoss: 0.818328\n",
      "Train Epoch: 6 [5120/60000]\tLoss: 1.004647\n",
      "Train Epoch: 6 [5760/60000]\tLoss: 0.968525\n",
      "Train Epoch: 6 [6400/60000]\tLoss: 1.073236\n",
      "Train Epoch: 6 [7040/60000]\tLoss: 0.716527\n",
      "Train Epoch: 6 [7680/60000]\tLoss: 0.847974\n",
      "Train Epoch: 6 [8320/60000]\tLoss: 0.836806\n",
      "Train Epoch: 6 [8960/60000]\tLoss: 0.898027\n",
      "Train Epoch: 6 [9600/60000]\tLoss: 0.924902\n",
      "Train Epoch: 6 [10240/60000]\tLoss: 0.731201\n",
      "Train Epoch: 6 [10880/60000]\tLoss: 0.855983\n",
      "Train Epoch: 6 [11520/60000]\tLoss: 0.770916\n",
      "Train Epoch: 6 [12160/60000]\tLoss: 0.713170\n",
      "Train Epoch: 6 [12800/60000]\tLoss: 0.676823\n",
      "Train Epoch: 6 [13440/60000]\tLoss: 0.963825\n",
      "Train Epoch: 6 [14080/60000]\tLoss: 0.991649\n",
      "Train Epoch: 6 [14720/60000]\tLoss: 1.015738\n",
      "Train Epoch: 6 [15360/60000]\tLoss: 0.641557\n",
      "Train Epoch: 6 [16000/60000]\tLoss: 0.729970\n",
      "Train Epoch: 6 [16640/60000]\tLoss: 0.867333\n",
      "Train Epoch: 6 [17280/60000]\tLoss: 0.800158\n",
      "Train Epoch: 6 [17920/60000]\tLoss: 0.702087\n",
      "Train Epoch: 6 [18560/60000]\tLoss: 0.731575\n",
      "Train Epoch: 6 [19200/60000]\tLoss: 0.781432\n",
      "Train Epoch: 6 [19840/60000]\tLoss: 0.806799\n",
      "Train Epoch: 6 [20480/60000]\tLoss: 0.793859\n",
      "Train Epoch: 6 [21120/60000]\tLoss: 0.765462\n",
      "Train Epoch: 6 [21760/60000]\tLoss: 0.535177\n",
      "Train Epoch: 6 [22400/60000]\tLoss: 0.819789\n",
      "Train Epoch: 6 [23040/60000]\tLoss: 0.809894\n",
      "Train Epoch: 6 [23680/60000]\tLoss: 0.818225\n",
      "Train Epoch: 6 [24320/60000]\tLoss: 0.775402\n",
      "Train Epoch: 6 [24960/60000]\tLoss: 0.749277\n",
      "Train Epoch: 6 [25600/60000]\tLoss: 0.698035\n",
      "Train Epoch: 6 [26240/60000]\tLoss: 1.067928\n",
      "Train Epoch: 6 [26880/60000]\tLoss: 0.975052\n",
      "Train Epoch: 6 [27520/60000]\tLoss: 1.124091\n",
      "Train Epoch: 6 [28160/60000]\tLoss: 0.709107\n",
      "Train Epoch: 6 [28800/60000]\tLoss: 0.879825\n",
      "Train Epoch: 6 [29440/60000]\tLoss: 1.004803\n",
      "Train Epoch: 6 [30080/60000]\tLoss: 0.659246\n",
      "Train Epoch: 6 [30720/60000]\tLoss: 0.631480\n",
      "Train Epoch: 6 [31360/60000]\tLoss: 0.718031\n",
      "Train Epoch: 6 [32000/60000]\tLoss: 0.690139\n",
      "Train Epoch: 6 [32640/60000]\tLoss: 0.358599\n",
      "Train Epoch: 6 [33280/60000]\tLoss: 0.847149\n",
      "Train Epoch: 6 [33920/60000]\tLoss: 1.154930\n",
      "Train Epoch: 6 [34560/60000]\tLoss: 0.534467\n",
      "Train Epoch: 6 [35200/60000]\tLoss: 0.912759\n",
      "Train Epoch: 6 [35840/60000]\tLoss: 0.735198\n",
      "Train Epoch: 6 [36480/60000]\tLoss: 0.785595\n",
      "Train Epoch: 6 [37120/60000]\tLoss: 0.986365\n",
      "Train Epoch: 6 [37760/60000]\tLoss: 0.629288\n",
      "Train Epoch: 6 [38400/60000]\tLoss: 1.132264\n",
      "Train Epoch: 6 [39040/60000]\tLoss: 0.826356\n",
      "Train Epoch: 6 [39680/60000]\tLoss: 1.185945\n",
      "Train Epoch: 6 [40320/60000]\tLoss: 0.638093\n",
      "Train Epoch: 6 [40960/60000]\tLoss: 0.675251\n",
      "Train Epoch: 6 [41600/60000]\tLoss: 0.616141\n",
      "Train Epoch: 6 [42240/60000]\tLoss: 0.886870\n",
      "Train Epoch: 6 [42880/60000]\tLoss: 0.592504\n",
      "Train Epoch: 6 [43520/60000]\tLoss: 0.930483\n",
      "Train Epoch: 6 [44160/60000]\tLoss: 0.885715\n",
      "Train Epoch: 6 [44800/60000]\tLoss: 0.628814\n",
      "Train Epoch: 6 [45440/60000]\tLoss: 0.953888\n",
      "Train Epoch: 6 [46080/60000]\tLoss: 0.712011\n",
      "Train Epoch: 6 [46720/60000]\tLoss: 0.901839\n",
      "Train Epoch: 6 [47360/60000]\tLoss: 0.511903\n",
      "Train Epoch: 6 [48000/60000]\tLoss: 0.676131\n",
      "Train Epoch: 6 [48640/60000]\tLoss: 0.809257\n",
      "Train Epoch: 6 [49280/60000]\tLoss: 0.732681\n",
      "Train Epoch: 6 [49920/60000]\tLoss: 0.721982\n",
      "Train Epoch: 6 [50560/60000]\tLoss: 0.995669\n",
      "Train Epoch: 6 [51200/60000]\tLoss: 0.855910\n",
      "Train Epoch: 6 [51840/60000]\tLoss: 0.785907\n",
      "Train Epoch: 6 [52480/60000]\tLoss: 0.568087\n",
      "Train Epoch: 6 [53120/60000]\tLoss: 0.971926\n",
      "Train Epoch: 6 [53760/60000]\tLoss: 0.784591\n",
      "Train Epoch: 6 [54400/60000]\tLoss: 0.714889\n",
      "Train Epoch: 6 [55040/60000]\tLoss: 0.675424\n",
      "Train Epoch: 6 [55680/60000]\tLoss: 0.756204\n",
      "Train Epoch: 6 [56320/60000]\tLoss: 1.068887\n",
      "Train Epoch: 6 [56960/60000]\tLoss: 0.736332\n",
      "Train Epoch: 6 [57600/60000]\tLoss: 0.559588\n",
      "Train Epoch: 6 [58240/60000]\tLoss: 0.760710\n",
      "Train Epoch: 6 [58880/60000]\tLoss: 0.948340\n",
      "Train Epoch: 6 [59520/60000]\tLoss: 0.715977\n",
      "Train Epoch: 7 [0/60000]\tLoss: 0.836873\n",
      "Train Epoch: 7 [640/60000]\tLoss: 0.730074\n",
      "Train Epoch: 7 [1280/60000]\tLoss: 0.579649\n",
      "Train Epoch: 7 [1920/60000]\tLoss: 0.793165\n",
      "Train Epoch: 7 [2560/60000]\tLoss: 0.814851\n",
      "Train Epoch: 7 [3200/60000]\tLoss: 0.657671\n",
      "Train Epoch: 7 [3840/60000]\tLoss: 0.967095\n",
      "Train Epoch: 7 [4480/60000]\tLoss: 0.665462\n",
      "Train Epoch: 7 [5120/60000]\tLoss: 0.969247\n",
      "Train Epoch: 7 [5760/60000]\tLoss: 0.849102\n",
      "Train Epoch: 7 [6400/60000]\tLoss: 0.798564\n",
      "Train Epoch: 7 [7040/60000]\tLoss: 0.612097\n",
      "Train Epoch: 7 [7680/60000]\tLoss: 0.775195\n",
      "Train Epoch: 7 [8320/60000]\tLoss: 0.917542\n",
      "Train Epoch: 7 [8960/60000]\tLoss: 0.761744\n",
      "Train Epoch: 7 [9600/60000]\tLoss: 0.906465\n",
      "Train Epoch: 7 [10240/60000]\tLoss: 0.814168\n",
      "Train Epoch: 7 [10880/60000]\tLoss: 0.964347\n",
      "Train Epoch: 7 [11520/60000]\tLoss: 0.713079\n",
      "Train Epoch: 7 [12160/60000]\tLoss: 0.820535\n",
      "Train Epoch: 7 [12800/60000]\tLoss: 0.894904\n",
      "Train Epoch: 7 [13440/60000]\tLoss: 0.919652\n",
      "Train Epoch: 7 [14080/60000]\tLoss: 0.568980\n",
      "Train Epoch: 7 [14720/60000]\tLoss: 0.974789\n",
      "Train Epoch: 7 [15360/60000]\tLoss: 0.898156\n",
      "Train Epoch: 7 [16000/60000]\tLoss: 1.247682\n",
      "Train Epoch: 7 [16640/60000]\tLoss: 0.927810\n",
      "Train Epoch: 7 [17280/60000]\tLoss: 0.796879\n",
      "Train Epoch: 7 [17920/60000]\tLoss: 0.620646\n",
      "Train Epoch: 7 [18560/60000]\tLoss: 0.707177\n",
      "Train Epoch: 7 [19200/60000]\tLoss: 0.828469\n",
      "Train Epoch: 7 [19840/60000]\tLoss: 0.825688\n",
      "Train Epoch: 7 [20480/60000]\tLoss: 0.624529\n",
      "Train Epoch: 7 [21120/60000]\tLoss: 0.763522\n",
      "Train Epoch: 7 [21760/60000]\tLoss: 0.654986\n",
      "Train Epoch: 7 [22400/60000]\tLoss: 0.708494\n",
      "Train Epoch: 7 [23040/60000]\tLoss: 1.132541\n",
      "Train Epoch: 7 [23680/60000]\tLoss: 0.629671\n",
      "Train Epoch: 7 [24320/60000]\tLoss: 1.029169\n",
      "Train Epoch: 7 [24960/60000]\tLoss: 0.927015\n",
      "Train Epoch: 7 [25600/60000]\tLoss: 0.781330\n",
      "Train Epoch: 7 [26240/60000]\tLoss: 0.684344\n",
      "Train Epoch: 7 [26880/60000]\tLoss: 0.769995\n",
      "Train Epoch: 7 [27520/60000]\tLoss: 1.105579\n",
      "Train Epoch: 7 [28160/60000]\tLoss: 0.653985\n",
      "Train Epoch: 7 [28800/60000]\tLoss: 0.827721\n",
      "Train Epoch: 7 [29440/60000]\tLoss: 0.821754\n",
      "Train Epoch: 7 [30080/60000]\tLoss: 1.128153\n",
      "Train Epoch: 7 [30720/60000]\tLoss: 0.889632\n",
      "Train Epoch: 7 [31360/60000]\tLoss: 0.789883\n",
      "Train Epoch: 7 [32000/60000]\tLoss: 0.771456\n",
      "Train Epoch: 7 [32640/60000]\tLoss: 0.678079\n",
      "Train Epoch: 7 [33280/60000]\tLoss: 0.762925\n",
      "Train Epoch: 7 [33920/60000]\tLoss: 0.665202\n",
      "Train Epoch: 7 [34560/60000]\tLoss: 0.907937\n",
      "Train Epoch: 7 [35200/60000]\tLoss: 0.785139\n",
      "Train Epoch: 7 [35840/60000]\tLoss: 0.608680\n",
      "Train Epoch: 7 [36480/60000]\tLoss: 0.822626\n",
      "Train Epoch: 7 [37120/60000]\tLoss: 0.791881\n",
      "Train Epoch: 7 [37760/60000]\tLoss: 0.721647\n",
      "Train Epoch: 7 [38400/60000]\tLoss: 1.068495\n",
      "Train Epoch: 7 [39040/60000]\tLoss: 0.813926\n",
      "Train Epoch: 7 [39680/60000]\tLoss: 0.962380\n",
      "Train Epoch: 7 [40320/60000]\tLoss: 0.603423\n",
      "Train Epoch: 7 [40960/60000]\tLoss: 0.820672\n",
      "Train Epoch: 7 [41600/60000]\tLoss: 0.981990\n",
      "Train Epoch: 7 [42240/60000]\tLoss: 0.698394\n",
      "Train Epoch: 7 [42880/60000]\tLoss: 0.538461\n",
      "Train Epoch: 7 [43520/60000]\tLoss: 0.648117\n",
      "Train Epoch: 7 [44160/60000]\tLoss: 0.741049\n",
      "Train Epoch: 7 [44800/60000]\tLoss: 0.754975\n",
      "Train Epoch: 7 [45440/60000]\tLoss: 0.826752\n",
      "Train Epoch: 7 [46080/60000]\tLoss: 0.827711\n",
      "Train Epoch: 7 [46720/60000]\tLoss: 0.741144\n",
      "Train Epoch: 7 [47360/60000]\tLoss: 0.706742\n",
      "Train Epoch: 7 [48000/60000]\tLoss: 0.918867\n",
      "Train Epoch: 7 [48640/60000]\tLoss: 0.693821\n",
      "Train Epoch: 7 [49280/60000]\tLoss: 0.888158\n",
      "Train Epoch: 7 [49920/60000]\tLoss: 0.755745\n",
      "Train Epoch: 7 [50560/60000]\tLoss: 0.622845\n",
      "Train Epoch: 7 [51200/60000]\tLoss: 0.720294\n",
      "Train Epoch: 7 [51840/60000]\tLoss: 0.723678\n",
      "Train Epoch: 7 [52480/60000]\tLoss: 1.181976\n",
      "Train Epoch: 7 [53120/60000]\tLoss: 0.637435\n",
      "Train Epoch: 7 [53760/60000]\tLoss: 0.665984\n",
      "Train Epoch: 7 [54400/60000]\tLoss: 1.038741\n",
      "Train Epoch: 7 [55040/60000]\tLoss: 1.032054\n",
      "Train Epoch: 7 [55680/60000]\tLoss: 0.875700\n",
      "Train Epoch: 7 [56320/60000]\tLoss: 1.038577\n",
      "Train Epoch: 7 [56960/60000]\tLoss: 0.870745\n",
      "Train Epoch: 7 [57600/60000]\tLoss: 1.022183\n",
      "Train Epoch: 7 [58240/60000]\tLoss: 0.735471\n",
      "Train Epoch: 7 [58880/60000]\tLoss: 0.746085\n",
      "Train Epoch: 7 [59520/60000]\tLoss: 0.752069\n",
      "Train Epoch: 8 [0/60000]\tLoss: 1.070384\n",
      "Train Epoch: 8 [640/60000]\tLoss: 0.591024\n",
      "Train Epoch: 8 [1280/60000]\tLoss: 0.865346\n",
      "Train Epoch: 8 [1920/60000]\tLoss: 0.909000\n",
      "Train Epoch: 8 [2560/60000]\tLoss: 0.858951\n",
      "Train Epoch: 8 [3200/60000]\tLoss: 0.794564\n",
      "Train Epoch: 8 [3840/60000]\tLoss: 0.717315\n",
      "Train Epoch: 8 [4480/60000]\tLoss: 0.920901\n",
      "Train Epoch: 8 [5120/60000]\tLoss: 0.830689\n",
      "Train Epoch: 8 [5760/60000]\tLoss: 0.787591\n",
      "Train Epoch: 8 [6400/60000]\tLoss: 0.908169\n",
      "Train Epoch: 8 [7040/60000]\tLoss: 0.976475\n",
      "Train Epoch: 8 [7680/60000]\tLoss: 0.776864\n",
      "Train Epoch: 8 [8320/60000]\tLoss: 0.824198\n",
      "Train Epoch: 8 [8960/60000]\tLoss: 0.783432\n",
      "Train Epoch: 8 [9600/60000]\tLoss: 0.855722\n",
      "Train Epoch: 8 [10240/60000]\tLoss: 0.845015\n",
      "Train Epoch: 8 [10880/60000]\tLoss: 0.668889\n",
      "Train Epoch: 8 [11520/60000]\tLoss: 0.946248\n",
      "Train Epoch: 8 [12160/60000]\tLoss: 0.874202\n",
      "Train Epoch: 8 [12800/60000]\tLoss: 0.811266\n",
      "Train Epoch: 8 [13440/60000]\tLoss: 0.926961\n",
      "Train Epoch: 8 [14080/60000]\tLoss: 0.680172\n",
      "Train Epoch: 8 [14720/60000]\tLoss: 0.710583\n",
      "Train Epoch: 8 [15360/60000]\tLoss: 0.734844\n",
      "Train Epoch: 8 [16000/60000]\tLoss: 0.571960\n",
      "Train Epoch: 8 [16640/60000]\tLoss: 0.901055\n",
      "Train Epoch: 8 [17280/60000]\tLoss: 0.997769\n",
      "Train Epoch: 8 [17920/60000]\tLoss: 0.728874\n",
      "Train Epoch: 8 [18560/60000]\tLoss: 0.684944\n",
      "Train Epoch: 8 [19200/60000]\tLoss: 0.725684\n",
      "Train Epoch: 8 [19840/60000]\tLoss: 0.527222\n",
      "Train Epoch: 8 [20480/60000]\tLoss: 0.761776\n",
      "Train Epoch: 8 [21120/60000]\tLoss: 1.044646\n",
      "Train Epoch: 8 [21760/60000]\tLoss: 0.740461\n",
      "Train Epoch: 8 [22400/60000]\tLoss: 0.757907\n",
      "Train Epoch: 8 [23040/60000]\tLoss: 0.801165\n",
      "Train Epoch: 8 [23680/60000]\tLoss: 0.534320\n",
      "Train Epoch: 8 [24320/60000]\tLoss: 0.677735\n",
      "Train Epoch: 8 [24960/60000]\tLoss: 1.018005\n",
      "Train Epoch: 8 [25600/60000]\tLoss: 0.831571\n",
      "Train Epoch: 8 [26240/60000]\tLoss: 0.992257\n",
      "Train Epoch: 8 [26880/60000]\tLoss: 1.034881\n",
      "Train Epoch: 8 [27520/60000]\tLoss: 0.725285\n",
      "Train Epoch: 8 [28160/60000]\tLoss: 0.750673\n",
      "Train Epoch: 8 [28800/60000]\tLoss: 0.608414\n",
      "Train Epoch: 8 [29440/60000]\tLoss: 0.691446\n",
      "Train Epoch: 8 [30080/60000]\tLoss: 0.682882\n",
      "Train Epoch: 8 [30720/60000]\tLoss: 0.885336\n",
      "Train Epoch: 8 [31360/60000]\tLoss: 0.553493\n",
      "Train Epoch: 8 [32000/60000]\tLoss: 1.067643\n",
      "Train Epoch: 8 [32640/60000]\tLoss: 1.032567\n",
      "Train Epoch: 8 [33280/60000]\tLoss: 1.042708\n",
      "Train Epoch: 8 [33920/60000]\tLoss: 0.851825\n",
      "Train Epoch: 8 [34560/60000]\tLoss: 0.938121\n",
      "Train Epoch: 8 [35200/60000]\tLoss: 0.766303\n",
      "Train Epoch: 8 [35840/60000]\tLoss: 0.735849\n",
      "Train Epoch: 8 [36480/60000]\tLoss: 0.838143\n",
      "Train Epoch: 8 [37120/60000]\tLoss: 0.814378\n",
      "Train Epoch: 8 [37760/60000]\tLoss: 0.840949\n",
      "Train Epoch: 8 [38400/60000]\tLoss: 1.193017\n",
      "Train Epoch: 8 [39040/60000]\tLoss: 0.823635\n",
      "Train Epoch: 8 [39680/60000]\tLoss: 0.818574\n",
      "Train Epoch: 8 [40320/60000]\tLoss: 0.903791\n",
      "Train Epoch: 8 [40960/60000]\tLoss: 0.860272\n",
      "Train Epoch: 8 [41600/60000]\tLoss: 0.974404\n",
      "Train Epoch: 8 [42240/60000]\tLoss: 0.734897\n",
      "Train Epoch: 8 [42880/60000]\tLoss: 0.595995\n",
      "Train Epoch: 8 [43520/60000]\tLoss: 0.805566\n",
      "Train Epoch: 8 [44160/60000]\tLoss: 0.703887\n",
      "Train Epoch: 8 [44800/60000]\tLoss: 0.947742\n",
      "Train Epoch: 8 [45440/60000]\tLoss: 0.686405\n",
      "Train Epoch: 8 [46080/60000]\tLoss: 0.763310\n",
      "Train Epoch: 8 [46720/60000]\tLoss: 0.800724\n",
      "Train Epoch: 8 [47360/60000]\tLoss: 1.043935\n",
      "Train Epoch: 8 [48000/60000]\tLoss: 1.032958\n",
      "Train Epoch: 8 [48640/60000]\tLoss: 0.704336\n",
      "Train Epoch: 8 [49280/60000]\tLoss: 1.018528\n",
      "Train Epoch: 8 [49920/60000]\tLoss: 0.779784\n",
      "Train Epoch: 8 [50560/60000]\tLoss: 0.441629\n",
      "Train Epoch: 8 [51200/60000]\tLoss: 0.588245\n",
      "Train Epoch: 8 [51840/60000]\tLoss: 1.208343\n",
      "Train Epoch: 8 [52480/60000]\tLoss: 0.751593\n",
      "Train Epoch: 8 [53120/60000]\tLoss: 0.890005\n",
      "Train Epoch: 8 [53760/60000]\tLoss: 1.324220\n",
      "Train Epoch: 8 [54400/60000]\tLoss: 0.872201\n",
      "Train Epoch: 8 [55040/60000]\tLoss: 0.734320\n",
      "Train Epoch: 8 [55680/60000]\tLoss: 0.704595\n",
      "Train Epoch: 8 [56320/60000]\tLoss: 0.773800\n",
      "Train Epoch: 8 [56960/60000]\tLoss: 0.795820\n",
      "Train Epoch: 8 [57600/60000]\tLoss: 0.923976\n",
      "Train Epoch: 8 [58240/60000]\tLoss: 0.903903\n",
      "Train Epoch: 8 [58880/60000]\tLoss: 0.796766\n",
      "Train Epoch: 8 [59520/60000]\tLoss: 0.858641\n",
      "Train Epoch: 9 [0/60000]\tLoss: 0.697249\n",
      "Train Epoch: 9 [640/60000]\tLoss: 0.750818\n",
      "Train Epoch: 9 [1280/60000]\tLoss: 0.978530\n",
      "Train Epoch: 9 [1920/60000]\tLoss: 1.016797\n",
      "Train Epoch: 9 [2560/60000]\tLoss: 0.734066\n",
      "Train Epoch: 9 [3200/60000]\tLoss: 0.640772\n",
      "Train Epoch: 9 [3840/60000]\tLoss: 0.787334\n",
      "Train Epoch: 9 [4480/60000]\tLoss: 0.717571\n",
      "Train Epoch: 9 [5120/60000]\tLoss: 0.813171\n",
      "Train Epoch: 9 [5760/60000]\tLoss: 0.915234\n",
      "Train Epoch: 9 [6400/60000]\tLoss: 0.907570\n",
      "Train Epoch: 9 [7040/60000]\tLoss: 0.501492\n",
      "Train Epoch: 9 [7680/60000]\tLoss: 1.028759\n",
      "Train Epoch: 9 [8320/60000]\tLoss: 0.980861\n",
      "Train Epoch: 9 [8960/60000]\tLoss: 0.741630\n",
      "Train Epoch: 9 [9600/60000]\tLoss: 0.774511\n",
      "Train Epoch: 9 [10240/60000]\tLoss: 0.794256\n",
      "Train Epoch: 9 [10880/60000]\tLoss: 1.008087\n",
      "Train Epoch: 9 [11520/60000]\tLoss: 0.753476\n",
      "Train Epoch: 9 [12160/60000]\tLoss: 0.810350\n",
      "Train Epoch: 9 [12800/60000]\tLoss: 0.639805\n",
      "Train Epoch: 9 [13440/60000]\tLoss: 0.732496\n",
      "Train Epoch: 9 [14080/60000]\tLoss: 0.889678\n",
      "Train Epoch: 9 [14720/60000]\tLoss: 0.857483\n",
      "Train Epoch: 9 [15360/60000]\tLoss: 0.595213\n",
      "Train Epoch: 9 [16000/60000]\tLoss: 0.738548\n",
      "Train Epoch: 9 [16640/60000]\tLoss: 0.743669\n",
      "Train Epoch: 9 [17280/60000]\tLoss: 0.833310\n",
      "Train Epoch: 9 [17920/60000]\tLoss: 0.837976\n",
      "Train Epoch: 9 [18560/60000]\tLoss: 0.762161\n",
      "Train Epoch: 9 [19200/60000]\tLoss: 0.747467\n",
      "Train Epoch: 9 [19840/60000]\tLoss: 0.537397\n",
      "Train Epoch: 9 [20480/60000]\tLoss: 0.844552\n",
      "Train Epoch: 9 [21120/60000]\tLoss: 0.807269\n",
      "Train Epoch: 9 [21760/60000]\tLoss: 0.934211\n",
      "Train Epoch: 9 [22400/60000]\tLoss: 0.700039\n",
      "Train Epoch: 9 [23040/60000]\tLoss: 0.762760\n",
      "Train Epoch: 9 [23680/60000]\tLoss: 0.850619\n",
      "Train Epoch: 9 [24320/60000]\tLoss: 0.775777\n",
      "Train Epoch: 9 [24960/60000]\tLoss: 0.815542\n",
      "Train Epoch: 9 [25600/60000]\tLoss: 0.679766\n",
      "Train Epoch: 9 [26240/60000]\tLoss: 0.855914\n",
      "Train Epoch: 9 [26880/60000]\tLoss: 0.803103\n",
      "Train Epoch: 9 [27520/60000]\tLoss: 0.937613\n",
      "Train Epoch: 9 [28160/60000]\tLoss: 0.482604\n",
      "Train Epoch: 9 [28800/60000]\tLoss: 1.008918\n",
      "Train Epoch: 9 [29440/60000]\tLoss: 0.781319\n",
      "Train Epoch: 9 [30080/60000]\tLoss: 0.756305\n",
      "Train Epoch: 9 [30720/60000]\tLoss: 0.771371\n",
      "Train Epoch: 9 [31360/60000]\tLoss: 0.821833\n",
      "Train Epoch: 9 [32000/60000]\tLoss: 0.916622\n",
      "Train Epoch: 9 [32640/60000]\tLoss: 0.924963\n",
      "Train Epoch: 9 [33280/60000]\tLoss: 1.193180\n",
      "Train Epoch: 9 [33920/60000]\tLoss: 0.627556\n",
      "Train Epoch: 9 [34560/60000]\tLoss: 0.982117\n",
      "Train Epoch: 9 [35200/60000]\tLoss: 0.883242\n",
      "Train Epoch: 9 [35840/60000]\tLoss: 0.865833\n",
      "Train Epoch: 9 [36480/60000]\tLoss: 1.003156\n",
      "Train Epoch: 9 [37120/60000]\tLoss: 0.950029\n",
      "Train Epoch: 9 [37760/60000]\tLoss: 0.930792\n",
      "Train Epoch: 9 [38400/60000]\tLoss: 0.814108\n",
      "Train Epoch: 9 [39040/60000]\tLoss: 1.049552\n",
      "Train Epoch: 9 [39680/60000]\tLoss: 0.823420\n",
      "Train Epoch: 9 [40320/60000]\tLoss: 0.868693\n",
      "Train Epoch: 9 [40960/60000]\tLoss: 0.948402\n",
      "Train Epoch: 9 [41600/60000]\tLoss: 0.822448\n",
      "Train Epoch: 9 [42240/60000]\tLoss: 0.958805\n",
      "Train Epoch: 9 [42880/60000]\tLoss: 0.958396\n",
      "Train Epoch: 9 [43520/60000]\tLoss: 0.961683\n",
      "Train Epoch: 9 [44160/60000]\tLoss: 0.685255\n",
      "Train Epoch: 9 [44800/60000]\tLoss: 0.731767\n",
      "Train Epoch: 9 [45440/60000]\tLoss: 0.639692\n",
      "Train Epoch: 9 [46080/60000]\tLoss: 1.056329\n",
      "Train Epoch: 9 [46720/60000]\tLoss: 0.852641\n",
      "Train Epoch: 9 [47360/60000]\tLoss: 0.964386\n",
      "Train Epoch: 9 [48000/60000]\tLoss: 0.617686\n",
      "Train Epoch: 9 [48640/60000]\tLoss: 0.777587\n",
      "Train Epoch: 9 [49280/60000]\tLoss: 0.667551\n",
      "Train Epoch: 9 [49920/60000]\tLoss: 0.983995\n",
      "Train Epoch: 9 [50560/60000]\tLoss: 0.700073\n",
      "Train Epoch: 9 [51200/60000]\tLoss: 0.756515\n",
      "Train Epoch: 9 [51840/60000]\tLoss: 0.610830\n",
      "Train Epoch: 9 [52480/60000]\tLoss: 0.825842\n",
      "Train Epoch: 9 [53120/60000]\tLoss: 0.716801\n",
      "Train Epoch: 9 [53760/60000]\tLoss: 0.723341\n",
      "Train Epoch: 9 [54400/60000]\tLoss: 1.024667\n",
      "Train Epoch: 9 [55040/60000]\tLoss: 0.655073\n",
      "Train Epoch: 9 [55680/60000]\tLoss: 0.757567\n",
      "Train Epoch: 9 [56320/60000]\tLoss: 0.910077\n",
      "Train Epoch: 9 [56960/60000]\tLoss: 1.046710\n",
      "Train Epoch: 9 [57600/60000]\tLoss: 0.872003\n",
      "Train Epoch: 9 [58240/60000]\tLoss: 0.894319\n",
      "Train Epoch: 9 [58880/60000]\tLoss: 0.726169\n",
      "Train Epoch: 9 [59520/60000]\tLoss: 1.064182\n",
      "Train Epoch: 10 [0/60000]\tLoss: 1.111989\n",
      "Train Epoch: 10 [640/60000]\tLoss: 0.790461\n",
      "Train Epoch: 10 [1280/60000]\tLoss: 0.647816\n",
      "Train Epoch: 10 [1920/60000]\tLoss: 0.894139\n",
      "Train Epoch: 10 [2560/60000]\tLoss: 0.725084\n",
      "Train Epoch: 10 [3200/60000]\tLoss: 0.862700\n",
      "Train Epoch: 10 [3840/60000]\tLoss: 0.520189\n",
      "Train Epoch: 10 [4480/60000]\tLoss: 0.681717\n",
      "Train Epoch: 10 [5120/60000]\tLoss: 0.760842\n",
      "Train Epoch: 10 [5760/60000]\tLoss: 0.921725\n",
      "Train Epoch: 10 [6400/60000]\tLoss: 0.749364\n",
      "Train Epoch: 10 [7040/60000]\tLoss: 0.657942\n",
      "Train Epoch: 10 [7680/60000]\tLoss: 0.980969\n",
      "Train Epoch: 10 [8320/60000]\tLoss: 0.716513\n",
      "Train Epoch: 10 [8960/60000]\tLoss: 0.705323\n",
      "Train Epoch: 10 [9600/60000]\tLoss: 0.860077\n",
      "Train Epoch: 10 [10240/60000]\tLoss: 0.725224\n",
      "Train Epoch: 10 [10880/60000]\tLoss: 1.053501\n",
      "Train Epoch: 10 [11520/60000]\tLoss: 0.872677\n",
      "Train Epoch: 10 [12160/60000]\tLoss: 0.727112\n",
      "Train Epoch: 10 [12800/60000]\tLoss: 0.746891\n",
      "Train Epoch: 10 [13440/60000]\tLoss: 0.461738\n",
      "Train Epoch: 10 [14080/60000]\tLoss: 0.746294\n",
      "Train Epoch: 10 [14720/60000]\tLoss: 1.165793\n",
      "Train Epoch: 10 [15360/60000]\tLoss: 0.823271\n",
      "Train Epoch: 10 [16000/60000]\tLoss: 0.619582\n",
      "Train Epoch: 10 [16640/60000]\tLoss: 0.657976\n",
      "Train Epoch: 10 [17280/60000]\tLoss: 0.452566\n",
      "Train Epoch: 10 [17920/60000]\tLoss: 1.118085\n",
      "Train Epoch: 10 [18560/60000]\tLoss: 0.800958\n",
      "Train Epoch: 10 [19200/60000]\tLoss: 0.898510\n",
      "Train Epoch: 10 [19840/60000]\tLoss: 0.903366\n",
      "Train Epoch: 10 [20480/60000]\tLoss: 0.484624\n",
      "Train Epoch: 10 [21120/60000]\tLoss: 1.180540\n",
      "Train Epoch: 10 [21760/60000]\tLoss: 1.038386\n",
      "Train Epoch: 10 [22400/60000]\tLoss: 1.180991\n",
      "Train Epoch: 10 [23040/60000]\tLoss: 0.729666\n",
      "Train Epoch: 10 [23680/60000]\tLoss: 0.805276\n",
      "Train Epoch: 10 [24320/60000]\tLoss: 0.810587\n",
      "Train Epoch: 10 [24960/60000]\tLoss: 0.909455\n",
      "Train Epoch: 10 [25600/60000]\tLoss: 0.741595\n",
      "Train Epoch: 10 [26240/60000]\tLoss: 0.797786\n",
      "Train Epoch: 10 [26880/60000]\tLoss: 0.810104\n",
      "Train Epoch: 10 [27520/60000]\tLoss: 0.898608\n",
      "Train Epoch: 10 [28160/60000]\tLoss: 1.046159\n",
      "Train Epoch: 10 [28800/60000]\tLoss: 0.795686\n",
      "Train Epoch: 10 [29440/60000]\tLoss: 0.970411\n",
      "Train Epoch: 10 [30080/60000]\tLoss: 0.886735\n",
      "Train Epoch: 10 [30720/60000]\tLoss: 0.682093\n",
      "Train Epoch: 10 [31360/60000]\tLoss: 0.755426\n",
      "Train Epoch: 10 [32000/60000]\tLoss: 1.146763\n",
      "Train Epoch: 10 [32640/60000]\tLoss: 0.652577\n",
      "Train Epoch: 10 [33280/60000]\tLoss: 0.877478\n",
      "Train Epoch: 10 [33920/60000]\tLoss: 0.961964\n",
      "Train Epoch: 10 [34560/60000]\tLoss: 0.722781\n",
      "Train Epoch: 10 [35200/60000]\tLoss: 0.606165\n",
      "Train Epoch: 10 [35840/60000]\tLoss: 0.757620\n",
      "Train Epoch: 10 [36480/60000]\tLoss: 0.912372\n",
      "Train Epoch: 10 [37120/60000]\tLoss: 1.071280\n",
      "Train Epoch: 10 [37760/60000]\tLoss: 0.764063\n",
      "Train Epoch: 10 [38400/60000]\tLoss: 0.742649\n",
      "Train Epoch: 10 [39040/60000]\tLoss: 0.802211\n",
      "Train Epoch: 10 [39680/60000]\tLoss: 0.887869\n",
      "Train Epoch: 10 [40320/60000]\tLoss: 0.688305\n",
      "Train Epoch: 10 [40960/60000]\tLoss: 0.874964\n",
      "Train Epoch: 10 [41600/60000]\tLoss: 0.861844\n",
      "Train Epoch: 10 [42240/60000]\tLoss: 0.696399\n",
      "Train Epoch: 10 [42880/60000]\tLoss: 0.869835\n",
      "Train Epoch: 10 [43520/60000]\tLoss: 1.122826\n",
      "Train Epoch: 10 [44160/60000]\tLoss: 0.811591\n",
      "Train Epoch: 10 [44800/60000]\tLoss: 0.543324\n",
      "Train Epoch: 10 [45440/60000]\tLoss: 1.111159\n",
      "Train Epoch: 10 [46080/60000]\tLoss: 0.818955\n",
      "Train Epoch: 10 [46720/60000]\tLoss: 0.877702\n",
      "Train Epoch: 10 [47360/60000]\tLoss: 0.531569\n",
      "Train Epoch: 10 [48000/60000]\tLoss: 0.800742\n",
      "Train Epoch: 10 [48640/60000]\tLoss: 0.624667\n",
      "Train Epoch: 10 [49280/60000]\tLoss: 0.749726\n",
      "Train Epoch: 10 [49920/60000]\tLoss: 0.881554\n",
      "Train Epoch: 10 [50560/60000]\tLoss: 1.363397\n",
      "Train Epoch: 10 [51200/60000]\tLoss: 0.752130\n",
      "Train Epoch: 10 [51840/60000]\tLoss: 0.875981\n",
      "Train Epoch: 10 [52480/60000]\tLoss: 0.748618\n",
      "Train Epoch: 10 [53120/60000]\tLoss: 1.057534\n",
      "Train Epoch: 10 [53760/60000]\tLoss: 0.683145\n",
      "Train Epoch: 10 [54400/60000]\tLoss: 0.861160\n",
      "Train Epoch: 10 [55040/60000]\tLoss: 0.867552\n",
      "Train Epoch: 10 [55680/60000]\tLoss: 0.463710\n",
      "Train Epoch: 10 [56320/60000]\tLoss: 0.725792\n",
      "Train Epoch: 10 [56960/60000]\tLoss: 0.940662\n",
      "Train Epoch: 10 [57600/60000]\tLoss: 0.862784\n",
      "Train Epoch: 10 [58240/60000]\tLoss: 1.005865\n",
      "Train Epoch: 10 [58880/60000]\tLoss: 0.590445\n",
      "Train Epoch: 10 [59520/60000]\tLoss: 0.964449\n",
      "\n",
      "Test set: Avg. loss: 0.2793, Accuracy: 9252/10000 (93%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and test Model 1\n",
    "\n",
    "# Create network\n",
    "model = Net()\n",
    "# Initialize model weights\n",
    "model.apply(weights_init)\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "# Get initial performance\n",
    "test(model)\n",
    "# Train for ten epochs\n",
    "n_epochs = 10\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch, model)\n",
    "accuracy1 = test(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 175139,
     "status": "ok",
     "timestamp": 1730967071190,
     "user": {
      "displayName": "Hsin-En Tsai",
      "userId": "01830060851848219976"
     },
     "user_tz": -480
    },
    "id": "r2PVnghrmr0F",
    "outputId": "18908b6a-a9d3-4ae6-e9d4-88501ff15661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.4400, Accuracy: 1032/10000 (10%)\n",
      "\n",
      "Train Epoch: 1 [0/60000]\tLoss: 2.450335\n",
      "Train Epoch: 1 [640/60000]\tLoss: 2.239987\n",
      "Train Epoch: 1 [1280/60000]\tLoss: 2.032527\n",
      "Train Epoch: 1 [1920/60000]\tLoss: 2.031553\n",
      "Train Epoch: 1 [2560/60000]\tLoss: 1.864995\n",
      "Train Epoch: 1 [3200/60000]\tLoss: 1.588605\n",
      "Train Epoch: 1 [3840/60000]\tLoss: 1.526454\n",
      "Train Epoch: 1 [4480/60000]\tLoss: 1.255484\n",
      "Train Epoch: 1 [5120/60000]\tLoss: 1.039349\n",
      "Train Epoch: 1 [5760/60000]\tLoss: 1.115046\n",
      "Train Epoch: 1 [6400/60000]\tLoss: 0.866646\n",
      "Train Epoch: 1 [7040/60000]\tLoss: 0.980188\n",
      "Train Epoch: 1 [7680/60000]\tLoss: 0.953560\n",
      "Train Epoch: 1 [8320/60000]\tLoss: 0.968576\n",
      "Train Epoch: 1 [8960/60000]\tLoss: 0.895470\n",
      "Train Epoch: 1 [9600/60000]\tLoss: 0.639581\n",
      "Train Epoch: 1 [10240/60000]\tLoss: 0.869754\n",
      "Train Epoch: 1 [10880/60000]\tLoss: 0.977854\n",
      "Train Epoch: 1 [11520/60000]\tLoss: 0.824896\n",
      "Train Epoch: 1 [12160/60000]\tLoss: 0.595552\n",
      "Train Epoch: 1 [12800/60000]\tLoss: 0.584257\n",
      "Train Epoch: 1 [13440/60000]\tLoss: 0.573234\n",
      "Train Epoch: 1 [14080/60000]\tLoss: 0.874059\n",
      "Train Epoch: 1 [14720/60000]\tLoss: 0.447335\n",
      "Train Epoch: 1 [15360/60000]\tLoss: 0.571318\n",
      "Train Epoch: 1 [16000/60000]\tLoss: 0.774469\n",
      "Train Epoch: 1 [16640/60000]\tLoss: 0.669530\n",
      "Train Epoch: 1 [17280/60000]\tLoss: 0.648857\n",
      "Train Epoch: 1 [17920/60000]\tLoss: 0.563867\n",
      "Train Epoch: 1 [18560/60000]\tLoss: 0.638942\n",
      "Train Epoch: 1 [19200/60000]\tLoss: 0.621978\n",
      "Train Epoch: 1 [19840/60000]\tLoss: 0.430704\n",
      "Train Epoch: 1 [20480/60000]\tLoss: 0.482963\n",
      "Train Epoch: 1 [21120/60000]\tLoss: 0.656897\n",
      "Train Epoch: 1 [21760/60000]\tLoss: 0.328952\n",
      "Train Epoch: 1 [22400/60000]\tLoss: 0.543753\n",
      "Train Epoch: 1 [23040/60000]\tLoss: 0.407715\n",
      "Train Epoch: 1 [23680/60000]\tLoss: 0.562737\n",
      "Train Epoch: 1 [24320/60000]\tLoss: 0.559678\n",
      "Train Epoch: 1 [24960/60000]\tLoss: 0.494543\n",
      "Train Epoch: 1 [25600/60000]\tLoss: 0.687892\n",
      "Train Epoch: 1 [26240/60000]\tLoss: 0.615472\n",
      "Train Epoch: 1 [26880/60000]\tLoss: 0.366568\n",
      "Train Epoch: 1 [27520/60000]\tLoss: 0.463845\n",
      "Train Epoch: 1 [28160/60000]\tLoss: 0.200226\n",
      "Train Epoch: 1 [28800/60000]\tLoss: 0.442883\n",
      "Train Epoch: 1 [29440/60000]\tLoss: 0.393042\n",
      "Train Epoch: 1 [30080/60000]\tLoss: 0.482607\n",
      "Train Epoch: 1 [30720/60000]\tLoss: 0.410411\n",
      "Train Epoch: 1 [31360/60000]\tLoss: 0.434853\n",
      "Train Epoch: 1 [32000/60000]\tLoss: 0.328816\n",
      "Train Epoch: 1 [32640/60000]\tLoss: 0.250859\n",
      "Train Epoch: 1 [33280/60000]\tLoss: 0.312610\n",
      "Train Epoch: 1 [33920/60000]\tLoss: 0.438716\n",
      "Train Epoch: 1 [34560/60000]\tLoss: 0.288042\n",
      "Train Epoch: 1 [35200/60000]\tLoss: 0.167666\n",
      "Train Epoch: 1 [35840/60000]\tLoss: 0.303291\n",
      "Train Epoch: 1 [36480/60000]\tLoss: 0.408220\n",
      "Train Epoch: 1 [37120/60000]\tLoss: 0.428235\n",
      "Train Epoch: 1 [37760/60000]\tLoss: 0.311538\n",
      "Train Epoch: 1 [38400/60000]\tLoss: 0.186896\n",
      "Train Epoch: 1 [39040/60000]\tLoss: 0.359491\n",
      "Train Epoch: 1 [39680/60000]\tLoss: 0.319253\n",
      "Train Epoch: 1 [40320/60000]\tLoss: 0.408683\n",
      "Train Epoch: 1 [40960/60000]\tLoss: 0.556390\n",
      "Train Epoch: 1 [41600/60000]\tLoss: 0.421799\n",
      "Train Epoch: 1 [42240/60000]\tLoss: 0.280112\n",
      "Train Epoch: 1 [42880/60000]\tLoss: 0.418040\n",
      "Train Epoch: 1 [43520/60000]\tLoss: 0.310961\n",
      "Train Epoch: 1 [44160/60000]\tLoss: 0.247850\n",
      "Train Epoch: 1 [44800/60000]\tLoss: 0.157038\n",
      "Train Epoch: 1 [45440/60000]\tLoss: 0.292447\n",
      "Train Epoch: 1 [46080/60000]\tLoss: 0.206309\n",
      "Train Epoch: 1 [46720/60000]\tLoss: 0.373880\n",
      "Train Epoch: 1 [47360/60000]\tLoss: 0.255974\n",
      "Train Epoch: 1 [48000/60000]\tLoss: 0.437812\n",
      "Train Epoch: 1 [48640/60000]\tLoss: 0.286393\n",
      "Train Epoch: 1 [49280/60000]\tLoss: 0.224875\n",
      "Train Epoch: 1 [49920/60000]\tLoss: 0.300683\n",
      "Train Epoch: 1 [50560/60000]\tLoss: 0.413812\n",
      "Train Epoch: 1 [51200/60000]\tLoss: 0.263698\n",
      "Train Epoch: 1 [51840/60000]\tLoss: 0.376455\n",
      "Train Epoch: 1 [52480/60000]\tLoss: 0.257169\n",
      "Train Epoch: 1 [53120/60000]\tLoss: 0.303941\n",
      "Train Epoch: 1 [53760/60000]\tLoss: 0.295453\n",
      "Train Epoch: 1 [54400/60000]\tLoss: 0.138967\n",
      "Train Epoch: 1 [55040/60000]\tLoss: 0.463330\n",
      "Train Epoch: 1 [55680/60000]\tLoss: 0.216237\n",
      "Train Epoch: 1 [56320/60000]\tLoss: 0.171184\n",
      "Train Epoch: 1 [56960/60000]\tLoss: 0.302994\n",
      "Train Epoch: 1 [57600/60000]\tLoss: 0.484782\n",
      "Train Epoch: 1 [58240/60000]\tLoss: 0.238157\n",
      "Train Epoch: 1 [58880/60000]\tLoss: 0.287120\n",
      "Train Epoch: 1 [59520/60000]\tLoss: 0.246462\n",
      "Train Epoch: 2 [0/60000]\tLoss: 0.271606\n",
      "Train Epoch: 2 [640/60000]\tLoss: 0.184124\n",
      "Train Epoch: 2 [1280/60000]\tLoss: 0.265296\n",
      "Train Epoch: 2 [1920/60000]\tLoss: 0.167022\n",
      "Train Epoch: 2 [2560/60000]\tLoss: 0.276982\n",
      "Train Epoch: 2 [3200/60000]\tLoss: 0.179569\n",
      "Train Epoch: 2 [3840/60000]\tLoss: 0.516630\n",
      "Train Epoch: 2 [4480/60000]\tLoss: 0.218271\n",
      "Train Epoch: 2 [5120/60000]\tLoss: 0.327189\n",
      "Train Epoch: 2 [5760/60000]\tLoss: 0.515118\n",
      "Train Epoch: 2 [6400/60000]\tLoss: 0.239707\n",
      "Train Epoch: 2 [7040/60000]\tLoss: 0.208848\n",
      "Train Epoch: 2 [7680/60000]\tLoss: 0.220932\n",
      "Train Epoch: 2 [8320/60000]\tLoss: 0.234563\n",
      "Train Epoch: 2 [8960/60000]\tLoss: 0.232082\n",
      "Train Epoch: 2 [9600/60000]\tLoss: 0.188357\n",
      "Train Epoch: 2 [10240/60000]\tLoss: 0.226974\n",
      "Train Epoch: 2 [10880/60000]\tLoss: 0.181782\n",
      "Train Epoch: 2 [11520/60000]\tLoss: 0.155603\n",
      "Train Epoch: 2 [12160/60000]\tLoss: 0.297712\n",
      "Train Epoch: 2 [12800/60000]\tLoss: 0.103772\n",
      "Train Epoch: 2 [13440/60000]\tLoss: 0.136594\n",
      "Train Epoch: 2 [14080/60000]\tLoss: 0.411958\n",
      "Train Epoch: 2 [14720/60000]\tLoss: 0.223993\n",
      "Train Epoch: 2 [15360/60000]\tLoss: 0.111982\n",
      "Train Epoch: 2 [16000/60000]\tLoss: 0.283479\n",
      "Train Epoch: 2 [16640/60000]\tLoss: 0.141140\n",
      "Train Epoch: 2 [17280/60000]\tLoss: 0.176415\n",
      "Train Epoch: 2 [17920/60000]\tLoss: 0.287736\n",
      "Train Epoch: 2 [18560/60000]\tLoss: 0.199715\n",
      "Train Epoch: 2 [19200/60000]\tLoss: 0.412576\n",
      "Train Epoch: 2 [19840/60000]\tLoss: 0.144097\n",
      "Train Epoch: 2 [20480/60000]\tLoss: 0.093536\n",
      "Train Epoch: 2 [21120/60000]\tLoss: 0.356795\n",
      "Train Epoch: 2 [21760/60000]\tLoss: 0.290550\n",
      "Train Epoch: 2 [22400/60000]\tLoss: 0.263696\n",
      "Train Epoch: 2 [23040/60000]\tLoss: 0.271596\n",
      "Train Epoch: 2 [23680/60000]\tLoss: 0.239569\n",
      "Train Epoch: 2 [24320/60000]\tLoss: 0.311029\n",
      "Train Epoch: 2 [24960/60000]\tLoss: 0.289425\n",
      "Train Epoch: 2 [25600/60000]\tLoss: 0.174689\n",
      "Train Epoch: 2 [26240/60000]\tLoss: 0.185692\n",
      "Train Epoch: 2 [26880/60000]\tLoss: 0.234117\n",
      "Train Epoch: 2 [27520/60000]\tLoss: 0.255095\n",
      "Train Epoch: 2 [28160/60000]\tLoss: 0.197647\n",
      "Train Epoch: 2 [28800/60000]\tLoss: 0.144448\n",
      "Train Epoch: 2 [29440/60000]\tLoss: 0.157632\n",
      "Train Epoch: 2 [30080/60000]\tLoss: 0.169380\n",
      "Train Epoch: 2 [30720/60000]\tLoss: 0.220582\n",
      "Train Epoch: 2 [31360/60000]\tLoss: 0.138417\n",
      "Train Epoch: 2 [32000/60000]\tLoss: 0.142324\n",
      "Train Epoch: 2 [32640/60000]\tLoss: 0.193713\n",
      "Train Epoch: 2 [33280/60000]\tLoss: 0.191392\n",
      "Train Epoch: 2 [33920/60000]\tLoss: 0.403481\n",
      "Train Epoch: 2 [34560/60000]\tLoss: 0.292437\n",
      "Train Epoch: 2 [35200/60000]\tLoss: 0.359479\n",
      "Train Epoch: 2 [35840/60000]\tLoss: 0.259502\n",
      "Train Epoch: 2 [36480/60000]\tLoss: 0.065782\n",
      "Train Epoch: 2 [37120/60000]\tLoss: 0.122746\n",
      "Train Epoch: 2 [37760/60000]\tLoss: 0.149393\n",
      "Train Epoch: 2 [38400/60000]\tLoss: 0.216948\n",
      "Train Epoch: 2 [39040/60000]\tLoss: 0.138945\n",
      "Train Epoch: 2 [39680/60000]\tLoss: 0.247073\n",
      "Train Epoch: 2 [40320/60000]\tLoss: 0.218289\n",
      "Train Epoch: 2 [40960/60000]\tLoss: 0.086717\n",
      "Train Epoch: 2 [41600/60000]\tLoss: 0.142945\n",
      "Train Epoch: 2 [42240/60000]\tLoss: 0.091542\n",
      "Train Epoch: 2 [42880/60000]\tLoss: 0.337744\n",
      "Train Epoch: 2 [43520/60000]\tLoss: 0.295244\n",
      "Train Epoch: 2 [44160/60000]\tLoss: 0.074951\n",
      "Train Epoch: 2 [44800/60000]\tLoss: 0.311942\n",
      "Train Epoch: 2 [45440/60000]\tLoss: 0.209337\n",
      "Train Epoch: 2 [46080/60000]\tLoss: 0.152241\n",
      "Train Epoch: 2 [46720/60000]\tLoss: 0.127881\n",
      "Train Epoch: 2 [47360/60000]\tLoss: 0.256590\n",
      "Train Epoch: 2 [48000/60000]\tLoss: 0.123455\n",
      "Train Epoch: 2 [48640/60000]\tLoss: 0.119216\n",
      "Train Epoch: 2 [49280/60000]\tLoss: 0.301238\n",
      "Train Epoch: 2 [49920/60000]\tLoss: 0.110015\n",
      "Train Epoch: 2 [50560/60000]\tLoss: 0.198407\n",
      "Train Epoch: 2 [51200/60000]\tLoss: 0.127779\n",
      "Train Epoch: 2 [51840/60000]\tLoss: 0.067762\n",
      "Train Epoch: 2 [52480/60000]\tLoss: 0.337171\n",
      "Train Epoch: 2 [53120/60000]\tLoss: 0.315964\n",
      "Train Epoch: 2 [53760/60000]\tLoss: 0.407602\n",
      "Train Epoch: 2 [54400/60000]\tLoss: 0.083110\n",
      "Train Epoch: 2 [55040/60000]\tLoss: 0.219131\n",
      "Train Epoch: 2 [55680/60000]\tLoss: 0.144390\n",
      "Train Epoch: 2 [56320/60000]\tLoss: 0.159311\n",
      "Train Epoch: 2 [56960/60000]\tLoss: 0.186470\n",
      "Train Epoch: 2 [57600/60000]\tLoss: 0.143641\n",
      "Train Epoch: 2 [58240/60000]\tLoss: 0.243589\n",
      "Train Epoch: 2 [58880/60000]\tLoss: 0.064524\n",
      "Train Epoch: 2 [59520/60000]\tLoss: 0.066639\n",
      "Train Epoch: 3 [0/60000]\tLoss: 0.346217\n",
      "Train Epoch: 3 [640/60000]\tLoss: 0.157991\n",
      "Train Epoch: 3 [1280/60000]\tLoss: 0.110945\n",
      "Train Epoch: 3 [1920/60000]\tLoss: 0.082532\n",
      "Train Epoch: 3 [2560/60000]\tLoss: 0.408867\n",
      "Train Epoch: 3 [3200/60000]\tLoss: 0.094358\n",
      "Train Epoch: 3 [3840/60000]\tLoss: 0.307260\n",
      "Train Epoch: 3 [4480/60000]\tLoss: 0.124844\n",
      "Train Epoch: 3 [5120/60000]\tLoss: 0.128187\n",
      "Train Epoch: 3 [5760/60000]\tLoss: 0.409160\n",
      "Train Epoch: 3 [6400/60000]\tLoss: 0.109221\n",
      "Train Epoch: 3 [7040/60000]\tLoss: 0.154786\n",
      "Train Epoch: 3 [7680/60000]\tLoss: 0.205835\n",
      "Train Epoch: 3 [8320/60000]\tLoss: 0.137154\n",
      "Train Epoch: 3 [8960/60000]\tLoss: 0.252835\n",
      "Train Epoch: 3 [9600/60000]\tLoss: 0.165713\n",
      "Train Epoch: 3 [10240/60000]\tLoss: 0.164433\n",
      "Train Epoch: 3 [10880/60000]\tLoss: 0.120144\n",
      "Train Epoch: 3 [11520/60000]\tLoss: 0.079699\n",
      "Train Epoch: 3 [12160/60000]\tLoss: 0.515595\n",
      "Train Epoch: 3 [12800/60000]\tLoss: 0.289898\n",
      "Train Epoch: 3 [13440/60000]\tLoss: 0.145958\n",
      "Train Epoch: 3 [14080/60000]\tLoss: 0.137987\n",
      "Train Epoch: 3 [14720/60000]\tLoss: 0.145154\n",
      "Train Epoch: 3 [15360/60000]\tLoss: 0.233895\n",
      "Train Epoch: 3 [16000/60000]\tLoss: 0.109869\n",
      "Train Epoch: 3 [16640/60000]\tLoss: 0.156189\n",
      "Train Epoch: 3 [17280/60000]\tLoss: 0.196475\n",
      "Train Epoch: 3 [17920/60000]\tLoss: 0.196548\n",
      "Train Epoch: 3 [18560/60000]\tLoss: 0.220595\n",
      "Train Epoch: 3 [19200/60000]\tLoss: 0.164845\n",
      "Train Epoch: 3 [19840/60000]\tLoss: 0.091832\n",
      "Train Epoch: 3 [20480/60000]\tLoss: 0.249820\n",
      "Train Epoch: 3 [21120/60000]\tLoss: 0.259262\n",
      "Train Epoch: 3 [21760/60000]\tLoss: 0.092964\n",
      "Train Epoch: 3 [22400/60000]\tLoss: 0.111510\n",
      "Train Epoch: 3 [23040/60000]\tLoss: 0.152977\n",
      "Train Epoch: 3 [23680/60000]\tLoss: 0.061213\n",
      "Train Epoch: 3 [24320/60000]\tLoss: 0.059074\n",
      "Train Epoch: 3 [24960/60000]\tLoss: 0.120795\n",
      "Train Epoch: 3 [25600/60000]\tLoss: 0.230964\n",
      "Train Epoch: 3 [26240/60000]\tLoss: 0.365539\n",
      "Train Epoch: 3 [26880/60000]\tLoss: 0.087280\n",
      "Train Epoch: 3 [27520/60000]\tLoss: 0.102606\n",
      "Train Epoch: 3 [28160/60000]\tLoss: 0.087239\n",
      "Train Epoch: 3 [28800/60000]\tLoss: 0.216134\n",
      "Train Epoch: 3 [29440/60000]\tLoss: 0.308252\n",
      "Train Epoch: 3 [30080/60000]\tLoss: 0.157154\n",
      "Train Epoch: 3 [30720/60000]\tLoss: 0.118276\n",
      "Train Epoch: 3 [31360/60000]\tLoss: 0.158652\n",
      "Train Epoch: 3 [32000/60000]\tLoss: 0.335654\n",
      "Train Epoch: 3 [32640/60000]\tLoss: 0.122526\n",
      "Train Epoch: 3 [33280/60000]\tLoss: 0.335850\n",
      "Train Epoch: 3 [33920/60000]\tLoss: 0.209200\n",
      "Train Epoch: 3 [34560/60000]\tLoss: 0.326057\n",
      "Train Epoch: 3 [35200/60000]\tLoss: 0.128467\n",
      "Train Epoch: 3 [35840/60000]\tLoss: 0.163782\n",
      "Train Epoch: 3 [36480/60000]\tLoss: 0.308253\n",
      "Train Epoch: 3 [37120/60000]\tLoss: 0.103008\n",
      "Train Epoch: 3 [37760/60000]\tLoss: 0.108861\n",
      "Train Epoch: 3 [38400/60000]\tLoss: 0.126183\n",
      "Train Epoch: 3 [39040/60000]\tLoss: 0.363059\n",
      "Train Epoch: 3 [39680/60000]\tLoss: 0.132217\n",
      "Train Epoch: 3 [40320/60000]\tLoss: 0.263323\n",
      "Train Epoch: 3 [40960/60000]\tLoss: 0.114683\n",
      "Train Epoch: 3 [41600/60000]\tLoss: 0.077963\n",
      "Train Epoch: 3 [42240/60000]\tLoss: 0.285804\n",
      "Train Epoch: 3 [42880/60000]\tLoss: 0.076257\n",
      "Train Epoch: 3 [43520/60000]\tLoss: 0.267053\n",
      "Train Epoch: 3 [44160/60000]\tLoss: 0.117714\n",
      "Train Epoch: 3 [44800/60000]\tLoss: 0.140748\n",
      "Train Epoch: 3 [45440/60000]\tLoss: 0.077247\n",
      "Train Epoch: 3 [46080/60000]\tLoss: 0.180788\n",
      "Train Epoch: 3 [46720/60000]\tLoss: 0.164292\n",
      "Train Epoch: 3 [47360/60000]\tLoss: 0.060906\n",
      "Train Epoch: 3 [48000/60000]\tLoss: 0.072053\n",
      "Train Epoch: 3 [48640/60000]\tLoss: 0.041075\n",
      "Train Epoch: 3 [49280/60000]\tLoss: 0.137758\n",
      "Train Epoch: 3 [49920/60000]\tLoss: 0.060974\n",
      "Train Epoch: 3 [50560/60000]\tLoss: 0.043649\n",
      "Train Epoch: 3 [51200/60000]\tLoss: 0.151960\n",
      "Train Epoch: 3 [51840/60000]\tLoss: 0.060589\n",
      "Train Epoch: 3 [52480/60000]\tLoss: 0.111782\n",
      "Train Epoch: 3 [53120/60000]\tLoss: 0.114924\n",
      "Train Epoch: 3 [53760/60000]\tLoss: 0.255436\n",
      "Train Epoch: 3 [54400/60000]\tLoss: 0.017774\n",
      "Train Epoch: 3 [55040/60000]\tLoss: 0.055287\n",
      "Train Epoch: 3 [55680/60000]\tLoss: 0.045382\n",
      "Train Epoch: 3 [56320/60000]\tLoss: 0.184248\n",
      "Train Epoch: 3 [56960/60000]\tLoss: 0.140597\n",
      "Train Epoch: 3 [57600/60000]\tLoss: 0.077707\n",
      "Train Epoch: 3 [58240/60000]\tLoss: 0.231861\n",
      "Train Epoch: 3 [58880/60000]\tLoss: 0.372367\n",
      "Train Epoch: 3 [59520/60000]\tLoss: 0.028697\n",
      "Train Epoch: 4 [0/60000]\tLoss: 0.077591\n",
      "Train Epoch: 4 [640/60000]\tLoss: 0.233114\n",
      "Train Epoch: 4 [1280/60000]\tLoss: 0.073851\n",
      "Train Epoch: 4 [1920/60000]\tLoss: 0.307353\n",
      "Train Epoch: 4 [2560/60000]\tLoss: 0.090258\n",
      "Train Epoch: 4 [3200/60000]\tLoss: 0.239730\n",
      "Train Epoch: 4 [3840/60000]\tLoss: 0.063798\n",
      "Train Epoch: 4 [4480/60000]\tLoss: 0.104884\n",
      "Train Epoch: 4 [5120/60000]\tLoss: 0.048681\n",
      "Train Epoch: 4 [5760/60000]\tLoss: 0.141663\n",
      "Train Epoch: 4 [6400/60000]\tLoss: 0.080530\n",
      "Train Epoch: 4 [7040/60000]\tLoss: 0.072802\n",
      "Train Epoch: 4 [7680/60000]\tLoss: 0.111288\n",
      "Train Epoch: 4 [8320/60000]\tLoss: 0.135012\n",
      "Train Epoch: 4 [8960/60000]\tLoss: 0.078123\n",
      "Train Epoch: 4 [9600/60000]\tLoss: 0.214632\n",
      "Train Epoch: 4 [10240/60000]\tLoss: 0.126197\n",
      "Train Epoch: 4 [10880/60000]\tLoss: 0.294598\n",
      "Train Epoch: 4 [11520/60000]\tLoss: 0.216717\n",
      "Train Epoch: 4 [12160/60000]\tLoss: 0.247927\n",
      "Train Epoch: 4 [12800/60000]\tLoss: 0.101181\n",
      "Train Epoch: 4 [13440/60000]\tLoss: 0.053335\n",
      "Train Epoch: 4 [14080/60000]\tLoss: 0.069699\n",
      "Train Epoch: 4 [14720/60000]\tLoss: 0.074111\n",
      "Train Epoch: 4 [15360/60000]\tLoss: 0.331698\n",
      "Train Epoch: 4 [16000/60000]\tLoss: 0.080516\n",
      "Train Epoch: 4 [16640/60000]\tLoss: 0.221511\n",
      "Train Epoch: 4 [17280/60000]\tLoss: 0.053562\n",
      "Train Epoch: 4 [17920/60000]\tLoss: 0.180405\n",
      "Train Epoch: 4 [18560/60000]\tLoss: 0.125220\n",
      "Train Epoch: 4 [19200/60000]\tLoss: 0.152223\n",
      "Train Epoch: 4 [19840/60000]\tLoss: 0.048382\n",
      "Train Epoch: 4 [20480/60000]\tLoss: 0.171285\n",
      "Train Epoch: 4 [21120/60000]\tLoss: 0.249466\n",
      "Train Epoch: 4 [21760/60000]\tLoss: 0.066045\n",
      "Train Epoch: 4 [22400/60000]\tLoss: 0.047172\n",
      "Train Epoch: 4 [23040/60000]\tLoss: 0.131772\n",
      "Train Epoch: 4 [23680/60000]\tLoss: 0.123755\n",
      "Train Epoch: 4 [24320/60000]\tLoss: 0.165353\n",
      "Train Epoch: 4 [24960/60000]\tLoss: 0.207530\n",
      "Train Epoch: 4 [25600/60000]\tLoss: 0.147794\n",
      "Train Epoch: 4 [26240/60000]\tLoss: 0.029917\n",
      "Train Epoch: 4 [26880/60000]\tLoss: 0.218704\n",
      "Train Epoch: 4 [27520/60000]\tLoss: 0.114425\n",
      "Train Epoch: 4 [28160/60000]\tLoss: 0.069126\n",
      "Train Epoch: 4 [28800/60000]\tLoss: 0.233358\n",
      "Train Epoch: 4 [29440/60000]\tLoss: 0.208015\n",
      "Train Epoch: 4 [30080/60000]\tLoss: 0.046869\n",
      "Train Epoch: 4 [30720/60000]\tLoss: 0.151207\n",
      "Train Epoch: 4 [31360/60000]\tLoss: 0.108297\n",
      "Train Epoch: 4 [32000/60000]\tLoss: 0.092153\n",
      "Train Epoch: 4 [32640/60000]\tLoss: 0.123456\n",
      "Train Epoch: 4 [33280/60000]\tLoss: 0.105460\n",
      "Train Epoch: 4 [33920/60000]\tLoss: 0.012846\n",
      "Train Epoch: 4 [34560/60000]\tLoss: 0.257560\n",
      "Train Epoch: 4 [35200/60000]\tLoss: 0.094685\n",
      "Train Epoch: 4 [35840/60000]\tLoss: 0.083315\n",
      "Train Epoch: 4 [36480/60000]\tLoss: 0.240605\n",
      "Train Epoch: 4 [37120/60000]\tLoss: 0.093234\n",
      "Train Epoch: 4 [37760/60000]\tLoss: 0.184666\n",
      "Train Epoch: 4 [38400/60000]\tLoss: 0.092224\n",
      "Train Epoch: 4 [39040/60000]\tLoss: 0.024058\n",
      "Train Epoch: 4 [39680/60000]\tLoss: 0.042916\n",
      "Train Epoch: 4 [40320/60000]\tLoss: 0.162329\n",
      "Train Epoch: 4 [40960/60000]\tLoss: 0.076210\n",
      "Train Epoch: 4 [41600/60000]\tLoss: 0.107351\n",
      "Train Epoch: 4 [42240/60000]\tLoss: 0.116839\n",
      "Train Epoch: 4 [42880/60000]\tLoss: 0.328872\n",
      "Train Epoch: 4 [43520/60000]\tLoss: 0.190903\n",
      "Train Epoch: 4 [44160/60000]\tLoss: 0.129651\n",
      "Train Epoch: 4 [44800/60000]\tLoss: 0.181703\n",
      "Train Epoch: 4 [45440/60000]\tLoss: 0.130490\n",
      "Train Epoch: 4 [46080/60000]\tLoss: 0.111528\n",
      "Train Epoch: 4 [46720/60000]\tLoss: 0.213369\n",
      "Train Epoch: 4 [47360/60000]\tLoss: 0.229379\n",
      "Train Epoch: 4 [48000/60000]\tLoss: 0.041228\n",
      "Train Epoch: 4 [48640/60000]\tLoss: 0.106841\n",
      "Train Epoch: 4 [49280/60000]\tLoss: 0.187341\n",
      "Train Epoch: 4 [49920/60000]\tLoss: 0.262297\n",
      "Train Epoch: 4 [50560/60000]\tLoss: 0.241525\n",
      "Train Epoch: 4 [51200/60000]\tLoss: 0.083886\n",
      "Train Epoch: 4 [51840/60000]\tLoss: 0.119105\n",
      "Train Epoch: 4 [52480/60000]\tLoss: 0.102093\n",
      "Train Epoch: 4 [53120/60000]\tLoss: 0.030704\n",
      "Train Epoch: 4 [53760/60000]\tLoss: 0.154287\n",
      "Train Epoch: 4 [54400/60000]\tLoss: 0.118307\n",
      "Train Epoch: 4 [55040/60000]\tLoss: 0.164410\n",
      "Train Epoch: 4 [55680/60000]\tLoss: 0.095974\n",
      "Train Epoch: 4 [56320/60000]\tLoss: 0.097266\n",
      "Train Epoch: 4 [56960/60000]\tLoss: 0.129958\n",
      "Train Epoch: 4 [57600/60000]\tLoss: 0.184697\n",
      "Train Epoch: 4 [58240/60000]\tLoss: 0.128807\n",
      "Train Epoch: 4 [58880/60000]\tLoss: 0.082586\n",
      "Train Epoch: 4 [59520/60000]\tLoss: 0.066966\n",
      "Train Epoch: 5 [0/60000]\tLoss: 0.034816\n",
      "Train Epoch: 5 [640/60000]\tLoss: 0.043745\n",
      "Train Epoch: 5 [1280/60000]\tLoss: 0.171550\n",
      "Train Epoch: 5 [1920/60000]\tLoss: 0.059233\n",
      "Train Epoch: 5 [2560/60000]\tLoss: 0.038811\n",
      "Train Epoch: 5 [3200/60000]\tLoss: 0.251813\n",
      "Train Epoch: 5 [3840/60000]\tLoss: 0.096059\n",
      "Train Epoch: 5 [4480/60000]\tLoss: 0.077271\n",
      "Train Epoch: 5 [5120/60000]\tLoss: 0.070503\n",
      "Train Epoch: 5 [5760/60000]\tLoss: 0.066510\n",
      "Train Epoch: 5 [6400/60000]\tLoss: 0.056707\n",
      "Train Epoch: 5 [7040/60000]\tLoss: 0.101310\n",
      "Train Epoch: 5 [7680/60000]\tLoss: 0.107961\n",
      "Train Epoch: 5 [8320/60000]\tLoss: 0.045750\n",
      "Train Epoch: 5 [8960/60000]\tLoss: 0.249591\n",
      "Train Epoch: 5 [9600/60000]\tLoss: 0.049521\n",
      "Train Epoch: 5 [10240/60000]\tLoss: 0.065824\n",
      "Train Epoch: 5 [10880/60000]\tLoss: 0.086896\n",
      "Train Epoch: 5 [11520/60000]\tLoss: 0.087833\n",
      "Train Epoch: 5 [12160/60000]\tLoss: 0.196375\n",
      "Train Epoch: 5 [12800/60000]\tLoss: 0.078913\n",
      "Train Epoch: 5 [13440/60000]\tLoss: 0.167941\n",
      "Train Epoch: 5 [14080/60000]\tLoss: 0.073987\n",
      "Train Epoch: 5 [14720/60000]\tLoss: 0.100672\n",
      "Train Epoch: 5 [15360/60000]\tLoss: 0.077233\n",
      "Train Epoch: 5 [16000/60000]\tLoss: 0.061620\n",
      "Train Epoch: 5 [16640/60000]\tLoss: 0.016408\n",
      "Train Epoch: 5 [17280/60000]\tLoss: 0.180142\n",
      "Train Epoch: 5 [17920/60000]\tLoss: 0.070695\n",
      "Train Epoch: 5 [18560/60000]\tLoss: 0.062244\n",
      "Train Epoch: 5 [19200/60000]\tLoss: 0.169372\n",
      "Train Epoch: 5 [19840/60000]\tLoss: 0.149255\n",
      "Train Epoch: 5 [20480/60000]\tLoss: 0.248241\n",
      "Train Epoch: 5 [21120/60000]\tLoss: 0.051428\n",
      "Train Epoch: 5 [21760/60000]\tLoss: 0.092758\n",
      "Train Epoch: 5 [22400/60000]\tLoss: 0.183805\n",
      "Train Epoch: 5 [23040/60000]\tLoss: 0.066196\n",
      "Train Epoch: 5 [23680/60000]\tLoss: 0.061281\n",
      "Train Epoch: 5 [24320/60000]\tLoss: 0.114308\n",
      "Train Epoch: 5 [24960/60000]\tLoss: 0.110674\n",
      "Train Epoch: 5 [25600/60000]\tLoss: 0.129035\n",
      "Train Epoch: 5 [26240/60000]\tLoss: 0.139797\n",
      "Train Epoch: 5 [26880/60000]\tLoss: 0.113336\n",
      "Train Epoch: 5 [27520/60000]\tLoss: 0.093153\n",
      "Train Epoch: 5 [28160/60000]\tLoss: 0.064040\n",
      "Train Epoch: 5 [28800/60000]\tLoss: 0.321601\n",
      "Train Epoch: 5 [29440/60000]\tLoss: 0.085123\n",
      "Train Epoch: 5 [30080/60000]\tLoss: 0.184200\n",
      "Train Epoch: 5 [30720/60000]\tLoss: 0.065543\n",
      "Train Epoch: 5 [31360/60000]\tLoss: 0.193700\n",
      "Train Epoch: 5 [32000/60000]\tLoss: 0.210045\n",
      "Train Epoch: 5 [32640/60000]\tLoss: 0.055157\n",
      "Train Epoch: 5 [33280/60000]\tLoss: 0.097251\n",
      "Train Epoch: 5 [33920/60000]\tLoss: 0.059308\n",
      "Train Epoch: 5 [34560/60000]\tLoss: 0.032155\n",
      "Train Epoch: 5 [35200/60000]\tLoss: 0.122062\n",
      "Train Epoch: 5 [35840/60000]\tLoss: 0.063648\n",
      "Train Epoch: 5 [36480/60000]\tLoss: 0.092880\n",
      "Train Epoch: 5 [37120/60000]\tLoss: 0.103086\n",
      "Train Epoch: 5 [37760/60000]\tLoss: 0.074875\n",
      "Train Epoch: 5 [38400/60000]\tLoss: 0.066009\n",
      "Train Epoch: 5 [39040/60000]\tLoss: 0.296163\n",
      "Train Epoch: 5 [39680/60000]\tLoss: 0.223278\n",
      "Train Epoch: 5 [40320/60000]\tLoss: 0.077142\n",
      "Train Epoch: 5 [40960/60000]\tLoss: 0.151969\n",
      "Train Epoch: 5 [41600/60000]\tLoss: 0.054925\n",
      "Train Epoch: 5 [42240/60000]\tLoss: 0.081807\n",
      "Train Epoch: 5 [42880/60000]\tLoss: 0.139291\n",
      "Train Epoch: 5 [43520/60000]\tLoss: 0.125039\n",
      "Train Epoch: 5 [44160/60000]\tLoss: 0.055372\n",
      "Train Epoch: 5 [44800/60000]\tLoss: 0.112716\n",
      "Train Epoch: 5 [45440/60000]\tLoss: 0.152958\n",
      "Train Epoch: 5 [46080/60000]\tLoss: 0.046811\n",
      "Train Epoch: 5 [46720/60000]\tLoss: 0.238954\n",
      "Train Epoch: 5 [47360/60000]\tLoss: 0.143825\n",
      "Train Epoch: 5 [48000/60000]\tLoss: 0.040475\n",
      "Train Epoch: 5 [48640/60000]\tLoss: 0.077416\n",
      "Train Epoch: 5 [49280/60000]\tLoss: 0.088108\n",
      "Train Epoch: 5 [49920/60000]\tLoss: 0.336143\n",
      "Train Epoch: 5 [50560/60000]\tLoss: 0.173917\n",
      "Train Epoch: 5 [51200/60000]\tLoss: 0.104047\n",
      "Train Epoch: 5 [51840/60000]\tLoss: 0.221202\n",
      "Train Epoch: 5 [52480/60000]\tLoss: 0.068430\n",
      "Train Epoch: 5 [53120/60000]\tLoss: 0.307084\n",
      "Train Epoch: 5 [53760/60000]\tLoss: 0.036991\n",
      "Train Epoch: 5 [54400/60000]\tLoss: 0.055678\n",
      "Train Epoch: 5 [55040/60000]\tLoss: 0.115035\n",
      "Train Epoch: 5 [55680/60000]\tLoss: 0.067153\n",
      "Train Epoch: 5 [56320/60000]\tLoss: 0.063560\n",
      "Train Epoch: 5 [56960/60000]\tLoss: 0.140046\n",
      "Train Epoch: 5 [57600/60000]\tLoss: 0.157218\n",
      "Train Epoch: 5 [58240/60000]\tLoss: 0.129392\n",
      "Train Epoch: 5 [58880/60000]\tLoss: 0.162897\n",
      "Train Epoch: 5 [59520/60000]\tLoss: 0.027001\n",
      "Train Epoch: 6 [0/60000]\tLoss: 0.124972\n",
      "Train Epoch: 6 [640/60000]\tLoss: 0.110875\n",
      "Train Epoch: 6 [1280/60000]\tLoss: 0.058521\n",
      "Train Epoch: 6 [1920/60000]\tLoss: 0.188540\n",
      "Train Epoch: 6 [2560/60000]\tLoss: 0.252274\n",
      "Train Epoch: 6 [3200/60000]\tLoss: 0.149392\n",
      "Train Epoch: 6 [3840/60000]\tLoss: 0.071046\n",
      "Train Epoch: 6 [4480/60000]\tLoss: 0.231303\n",
      "Train Epoch: 6 [5120/60000]\tLoss: 0.063224\n",
      "Train Epoch: 6 [5760/60000]\tLoss: 0.070674\n",
      "Train Epoch: 6 [6400/60000]\tLoss: 0.130988\n",
      "Train Epoch: 6 [7040/60000]\tLoss: 0.064212\n",
      "Train Epoch: 6 [7680/60000]\tLoss: 0.072922\n",
      "Train Epoch: 6 [8320/60000]\tLoss: 0.201941\n",
      "Train Epoch: 6 [8960/60000]\tLoss: 0.096151\n",
      "Train Epoch: 6 [9600/60000]\tLoss: 0.100834\n",
      "Train Epoch: 6 [10240/60000]\tLoss: 0.051852\n",
      "Train Epoch: 6 [10880/60000]\tLoss: 0.067159\n",
      "Train Epoch: 6 [11520/60000]\tLoss: 0.037442\n",
      "Train Epoch: 6 [12160/60000]\tLoss: 0.122206\n",
      "Train Epoch: 6 [12800/60000]\tLoss: 0.170482\n",
      "Train Epoch: 6 [13440/60000]\tLoss: 0.124719\n",
      "Train Epoch: 6 [14080/60000]\tLoss: 0.083524\n",
      "Train Epoch: 6 [14720/60000]\tLoss: 0.023032\n",
      "Train Epoch: 6 [15360/60000]\tLoss: 0.065786\n",
      "Train Epoch: 6 [16000/60000]\tLoss: 0.111387\n",
      "Train Epoch: 6 [16640/60000]\tLoss: 0.153630\n",
      "Train Epoch: 6 [17280/60000]\tLoss: 0.069216\n",
      "Train Epoch: 6 [17920/60000]\tLoss: 0.080096\n",
      "Train Epoch: 6 [18560/60000]\tLoss: 0.104792\n",
      "Train Epoch: 6 [19200/60000]\tLoss: 0.049995\n",
      "Train Epoch: 6 [19840/60000]\tLoss: 0.035452\n",
      "Train Epoch: 6 [20480/60000]\tLoss: 0.063333\n",
      "Train Epoch: 6 [21120/60000]\tLoss: 0.052084\n",
      "Train Epoch: 6 [21760/60000]\tLoss: 0.051781\n",
      "Train Epoch: 6 [22400/60000]\tLoss: 0.176251\n",
      "Train Epoch: 6 [23040/60000]\tLoss: 0.039486\n",
      "Train Epoch: 6 [23680/60000]\tLoss: 0.116416\n",
      "Train Epoch: 6 [24320/60000]\tLoss: 0.029811\n",
      "Train Epoch: 6 [24960/60000]\tLoss: 0.123154\n",
      "Train Epoch: 6 [25600/60000]\tLoss: 0.069740\n",
      "Train Epoch: 6 [26240/60000]\tLoss: 0.142203\n",
      "Train Epoch: 6 [26880/60000]\tLoss: 0.181093\n",
      "Train Epoch: 6 [27520/60000]\tLoss: 0.115805\n",
      "Train Epoch: 6 [28160/60000]\tLoss: 0.146863\n",
      "Train Epoch: 6 [28800/60000]\tLoss: 0.099629\n",
      "Train Epoch: 6 [29440/60000]\tLoss: 0.081235\n",
      "Train Epoch: 6 [30080/60000]\tLoss: 0.022982\n",
      "Train Epoch: 6 [30720/60000]\tLoss: 0.156234\n",
      "Train Epoch: 6 [31360/60000]\tLoss: 0.018093\n",
      "Train Epoch: 6 [32000/60000]\tLoss: 0.100880\n",
      "Train Epoch: 6 [32640/60000]\tLoss: 0.142643\n",
      "Train Epoch: 6 [33280/60000]\tLoss: 0.114432\n",
      "Train Epoch: 6 [33920/60000]\tLoss: 0.068755\n",
      "Train Epoch: 6 [34560/60000]\tLoss: 0.100681\n",
      "Train Epoch: 6 [35200/60000]\tLoss: 0.064556\n",
      "Train Epoch: 6 [35840/60000]\tLoss: 0.046692\n",
      "Train Epoch: 6 [36480/60000]\tLoss: 0.139608\n",
      "Train Epoch: 6 [37120/60000]\tLoss: 0.071111\n",
      "Train Epoch: 6 [37760/60000]\tLoss: 0.173132\n",
      "Train Epoch: 6 [38400/60000]\tLoss: 0.145212\n",
      "Train Epoch: 6 [39040/60000]\tLoss: 0.095983\n",
      "Train Epoch: 6 [39680/60000]\tLoss: 0.052680\n",
      "Train Epoch: 6 [40320/60000]\tLoss: 0.032919\n",
      "Train Epoch: 6 [40960/60000]\tLoss: 0.171599\n",
      "Train Epoch: 6 [41600/60000]\tLoss: 0.038290\n",
      "Train Epoch: 6 [42240/60000]\tLoss: 0.081932\n",
      "Train Epoch: 6 [42880/60000]\tLoss: 0.086916\n",
      "Train Epoch: 6 [43520/60000]\tLoss: 0.101130\n",
      "Train Epoch: 6 [44160/60000]\tLoss: 0.121244\n",
      "Train Epoch: 6 [44800/60000]\tLoss: 0.080647\n",
      "Train Epoch: 6 [45440/60000]\tLoss: 0.133913\n",
      "Train Epoch: 6 [46080/60000]\tLoss: 0.038243\n",
      "Train Epoch: 6 [46720/60000]\tLoss: 0.102282\n",
      "Train Epoch: 6 [47360/60000]\tLoss: 0.114718\n",
      "Train Epoch: 6 [48000/60000]\tLoss: 0.050648\n",
      "Train Epoch: 6 [48640/60000]\tLoss: 0.127831\n",
      "Train Epoch: 6 [49280/60000]\tLoss: 0.208290\n",
      "Train Epoch: 6 [49920/60000]\tLoss: 0.260291\n",
      "Train Epoch: 6 [50560/60000]\tLoss: 0.072608\n",
      "Train Epoch: 6 [51200/60000]\tLoss: 0.060423\n",
      "Train Epoch: 6 [51840/60000]\tLoss: 0.044183\n",
      "Train Epoch: 6 [52480/60000]\tLoss: 0.111891\n",
      "Train Epoch: 6 [53120/60000]\tLoss: 0.207914\n",
      "Train Epoch: 6 [53760/60000]\tLoss: 0.167637\n",
      "Train Epoch: 6 [54400/60000]\tLoss: 0.080682\n",
      "Train Epoch: 6 [55040/60000]\tLoss: 0.246641\n",
      "Train Epoch: 6 [55680/60000]\tLoss: 0.473073\n",
      "Train Epoch: 6 [56320/60000]\tLoss: 0.140490\n",
      "Train Epoch: 6 [56960/60000]\tLoss: 0.015194\n",
      "Train Epoch: 6 [57600/60000]\tLoss: 0.139849\n",
      "Train Epoch: 6 [58240/60000]\tLoss: 0.062487\n",
      "Train Epoch: 6 [58880/60000]\tLoss: 0.239828\n",
      "Train Epoch: 6 [59520/60000]\tLoss: 0.071479\n",
      "Train Epoch: 7 [0/60000]\tLoss: 0.092351\n",
      "Train Epoch: 7 [640/60000]\tLoss: 0.271923\n",
      "Train Epoch: 7 [1280/60000]\tLoss: 0.036654\n",
      "Train Epoch: 7 [1920/60000]\tLoss: 0.016376\n",
      "Train Epoch: 7 [2560/60000]\tLoss: 0.216702\n",
      "Train Epoch: 7 [3200/60000]\tLoss: 0.184188\n",
      "Train Epoch: 7 [3840/60000]\tLoss: 0.158782\n",
      "Train Epoch: 7 [4480/60000]\tLoss: 0.052167\n",
      "Train Epoch: 7 [5120/60000]\tLoss: 0.071723\n",
      "Train Epoch: 7 [5760/60000]\tLoss: 0.092670\n",
      "Train Epoch: 7 [6400/60000]\tLoss: 0.198190\n",
      "Train Epoch: 7 [7040/60000]\tLoss: 0.059553\n",
      "Train Epoch: 7 [7680/60000]\tLoss: 0.205908\n",
      "Train Epoch: 7 [8320/60000]\tLoss: 0.156316\n",
      "Train Epoch: 7 [8960/60000]\tLoss: 0.161283\n",
      "Train Epoch: 7 [9600/60000]\tLoss: 0.128898\n",
      "Train Epoch: 7 [10240/60000]\tLoss: 0.026582\n",
      "Train Epoch: 7 [10880/60000]\tLoss: 0.170362\n",
      "Train Epoch: 7 [11520/60000]\tLoss: 0.078962\n",
      "Train Epoch: 7 [12160/60000]\tLoss: 0.062647\n",
      "Train Epoch: 7 [12800/60000]\tLoss: 0.070935\n",
      "Train Epoch: 7 [13440/60000]\tLoss: 0.090472\n",
      "Train Epoch: 7 [14080/60000]\tLoss: 0.109503\n",
      "Train Epoch: 7 [14720/60000]\tLoss: 0.026840\n",
      "Train Epoch: 7 [15360/60000]\tLoss: 0.195579\n",
      "Train Epoch: 7 [16000/60000]\tLoss: 0.106150\n",
      "Train Epoch: 7 [16640/60000]\tLoss: 0.171518\n",
      "Train Epoch: 7 [17280/60000]\tLoss: 0.200686\n",
      "Train Epoch: 7 [17920/60000]\tLoss: 0.078938\n",
      "Train Epoch: 7 [18560/60000]\tLoss: 0.049828\n",
      "Train Epoch: 7 [19200/60000]\tLoss: 0.074925\n",
      "Train Epoch: 7 [19840/60000]\tLoss: 0.086160\n",
      "Train Epoch: 7 [20480/60000]\tLoss: 0.114646\n",
      "Train Epoch: 7 [21120/60000]\tLoss: 0.057713\n",
      "Train Epoch: 7 [21760/60000]\tLoss: 0.031525\n",
      "Train Epoch: 7 [22400/60000]\tLoss: 0.011330\n",
      "Train Epoch: 7 [23040/60000]\tLoss: 0.044903\n",
      "Train Epoch: 7 [23680/60000]\tLoss: 0.056050\n",
      "Train Epoch: 7 [24320/60000]\tLoss: 0.046335\n",
      "Train Epoch: 7 [24960/60000]\tLoss: 0.161080\n",
      "Train Epoch: 7 [25600/60000]\tLoss: 0.174138\n",
      "Train Epoch: 7 [26240/60000]\tLoss: 0.016335\n",
      "Train Epoch: 7 [26880/60000]\tLoss: 0.123351\n",
      "Train Epoch: 7 [27520/60000]\tLoss: 0.029736\n",
      "Train Epoch: 7 [28160/60000]\tLoss: 0.013721\n",
      "Train Epoch: 7 [28800/60000]\tLoss: 0.031910\n",
      "Train Epoch: 7 [29440/60000]\tLoss: 0.133150\n",
      "Train Epoch: 7 [30080/60000]\tLoss: 0.178782\n",
      "Train Epoch: 7 [30720/60000]\tLoss: 0.081130\n",
      "Train Epoch: 7 [31360/60000]\tLoss: 0.144731\n",
      "Train Epoch: 7 [32000/60000]\tLoss: 0.230729\n",
      "Train Epoch: 7 [32640/60000]\tLoss: 0.108346\n",
      "Train Epoch: 7 [33280/60000]\tLoss: 0.139009\n",
      "Train Epoch: 7 [33920/60000]\tLoss: 0.155103\n",
      "Train Epoch: 7 [34560/60000]\tLoss: 0.071628\n",
      "Train Epoch: 7 [35200/60000]\tLoss: 0.061268\n",
      "Train Epoch: 7 [35840/60000]\tLoss: 0.145851\n",
      "Train Epoch: 7 [36480/60000]\tLoss: 0.100637\n",
      "Train Epoch: 7 [37120/60000]\tLoss: 0.126718\n",
      "Train Epoch: 7 [37760/60000]\tLoss: 0.125415\n",
      "Train Epoch: 7 [38400/60000]\tLoss: 0.205736\n",
      "Train Epoch: 7 [39040/60000]\tLoss: 0.247856\n",
      "Train Epoch: 7 [39680/60000]\tLoss: 0.025609\n",
      "Train Epoch: 7 [40320/60000]\tLoss: 0.061439\n",
      "Train Epoch: 7 [40960/60000]\tLoss: 0.087884\n",
      "Train Epoch: 7 [41600/60000]\tLoss: 0.078197\n",
      "Train Epoch: 7 [42240/60000]\tLoss: 0.081529\n",
      "Train Epoch: 7 [42880/60000]\tLoss: 0.055521\n",
      "Train Epoch: 7 [43520/60000]\tLoss: 0.084723\n",
      "Train Epoch: 7 [44160/60000]\tLoss: 0.096819\n",
      "Train Epoch: 7 [44800/60000]\tLoss: 0.231322\n",
      "Train Epoch: 7 [45440/60000]\tLoss: 0.055191\n",
      "Train Epoch: 7 [46080/60000]\tLoss: 0.011471\n",
      "Train Epoch: 7 [46720/60000]\tLoss: 0.168243\n",
      "Train Epoch: 7 [47360/60000]\tLoss: 0.058781\n",
      "Train Epoch: 7 [48000/60000]\tLoss: 0.049478\n",
      "Train Epoch: 7 [48640/60000]\tLoss: 0.054502\n",
      "Train Epoch: 7 [49280/60000]\tLoss: 0.055791\n",
      "Train Epoch: 7 [49920/60000]\tLoss: 0.032864\n",
      "Train Epoch: 7 [50560/60000]\tLoss: 0.313740\n",
      "Train Epoch: 7 [51200/60000]\tLoss: 0.200329\n",
      "Train Epoch: 7 [51840/60000]\tLoss: 0.047307\n",
      "Train Epoch: 7 [52480/60000]\tLoss: 0.081624\n",
      "Train Epoch: 7 [53120/60000]\tLoss: 0.157212\n",
      "Train Epoch: 7 [53760/60000]\tLoss: 0.053395\n",
      "Train Epoch: 7 [54400/60000]\tLoss: 0.071509\n",
      "Train Epoch: 7 [55040/60000]\tLoss: 0.105504\n",
      "Train Epoch: 7 [55680/60000]\tLoss: 0.189732\n",
      "Train Epoch: 7 [56320/60000]\tLoss: 0.138379\n",
      "Train Epoch: 7 [56960/60000]\tLoss: 0.086615\n",
      "Train Epoch: 7 [57600/60000]\tLoss: 0.113435\n",
      "Train Epoch: 7 [58240/60000]\tLoss: 0.015007\n",
      "Train Epoch: 7 [58880/60000]\tLoss: 0.179087\n",
      "Train Epoch: 7 [59520/60000]\tLoss: 0.070228\n",
      "Train Epoch: 8 [0/60000]\tLoss: 0.109471\n",
      "Train Epoch: 8 [640/60000]\tLoss: 0.078050\n",
      "Train Epoch: 8 [1280/60000]\tLoss: 0.055363\n",
      "Train Epoch: 8 [1920/60000]\tLoss: 0.019360\n",
      "Train Epoch: 8 [2560/60000]\tLoss: 0.118634\n",
      "Train Epoch: 8 [3200/60000]\tLoss: 0.063481\n",
      "Train Epoch: 8 [3840/60000]\tLoss: 0.018142\n",
      "Train Epoch: 8 [4480/60000]\tLoss: 0.127232\n",
      "Train Epoch: 8 [5120/60000]\tLoss: 0.035462\n",
      "Train Epoch: 8 [5760/60000]\tLoss: 0.056633\n",
      "Train Epoch: 8 [6400/60000]\tLoss: 0.215173\n",
      "Train Epoch: 8 [7040/60000]\tLoss: 0.032242\n",
      "Train Epoch: 8 [7680/60000]\tLoss: 0.127061\n",
      "Train Epoch: 8 [8320/60000]\tLoss: 0.101475\n",
      "Train Epoch: 8 [8960/60000]\tLoss: 0.118680\n",
      "Train Epoch: 8 [9600/60000]\tLoss: 0.026387\n",
      "Train Epoch: 8 [10240/60000]\tLoss: 0.104351\n",
      "Train Epoch: 8 [10880/60000]\tLoss: 0.150315\n",
      "Train Epoch: 8 [11520/60000]\tLoss: 0.135811\n",
      "Train Epoch: 8 [12160/60000]\tLoss: 0.265930\n",
      "Train Epoch: 8 [12800/60000]\tLoss: 0.143515\n",
      "Train Epoch: 8 [13440/60000]\tLoss: 0.079589\n",
      "Train Epoch: 8 [14080/60000]\tLoss: 0.081345\n",
      "Train Epoch: 8 [14720/60000]\tLoss: 0.110788\n",
      "Train Epoch: 8 [15360/60000]\tLoss: 0.031109\n",
      "Train Epoch: 8 [16000/60000]\tLoss: 0.026751\n",
      "Train Epoch: 8 [16640/60000]\tLoss: 0.054633\n",
      "Train Epoch: 8 [17280/60000]\tLoss: 0.103086\n",
      "Train Epoch: 8 [17920/60000]\tLoss: 0.063599\n",
      "Train Epoch: 8 [18560/60000]\tLoss: 0.114853\n",
      "Train Epoch: 8 [19200/60000]\tLoss: 0.111041\n",
      "Train Epoch: 8 [19840/60000]\tLoss: 0.082519\n",
      "Train Epoch: 8 [20480/60000]\tLoss: 0.197857\n",
      "Train Epoch: 8 [21120/60000]\tLoss: 0.109276\n",
      "Train Epoch: 8 [21760/60000]\tLoss: 0.094157\n",
      "Train Epoch: 8 [22400/60000]\tLoss: 0.186748\n",
      "Train Epoch: 8 [23040/60000]\tLoss: 0.009063\n",
      "Train Epoch: 8 [23680/60000]\tLoss: 0.194135\n",
      "Train Epoch: 8 [24320/60000]\tLoss: 0.260124\n",
      "Train Epoch: 8 [24960/60000]\tLoss: 0.172829\n",
      "Train Epoch: 8 [25600/60000]\tLoss: 0.030874\n",
      "Train Epoch: 8 [26240/60000]\tLoss: 0.254664\n",
      "Train Epoch: 8 [26880/60000]\tLoss: 0.038704\n",
      "Train Epoch: 8 [27520/60000]\tLoss: 0.052495\n",
      "Train Epoch: 8 [28160/60000]\tLoss: 0.226439\n",
      "Train Epoch: 8 [28800/60000]\tLoss: 0.013745\n",
      "Train Epoch: 8 [29440/60000]\tLoss: 0.055244\n",
      "Train Epoch: 8 [30080/60000]\tLoss: 0.125563\n",
      "Train Epoch: 8 [30720/60000]\tLoss: 0.310127\n",
      "Train Epoch: 8 [31360/60000]\tLoss: 0.039917\n",
      "Train Epoch: 8 [32000/60000]\tLoss: 0.092860\n",
      "Train Epoch: 8 [32640/60000]\tLoss: 0.054132\n",
      "Train Epoch: 8 [33280/60000]\tLoss: 0.031126\n",
      "Train Epoch: 8 [33920/60000]\tLoss: 0.037207\n",
      "Train Epoch: 8 [34560/60000]\tLoss: 0.078095\n",
      "Train Epoch: 8 [35200/60000]\tLoss: 0.037587\n",
      "Train Epoch: 8 [35840/60000]\tLoss: 0.139470\n",
      "Train Epoch: 8 [36480/60000]\tLoss: 0.143700\n",
      "Train Epoch: 8 [37120/60000]\tLoss: 0.092440\n",
      "Train Epoch: 8 [37760/60000]\tLoss: 0.088969\n",
      "Train Epoch: 8 [38400/60000]\tLoss: 0.019736\n",
      "Train Epoch: 8 [39040/60000]\tLoss: 0.132875\n",
      "Train Epoch: 8 [39680/60000]\tLoss: 0.232654\n",
      "Train Epoch: 8 [40320/60000]\tLoss: 0.044647\n",
      "Train Epoch: 8 [40960/60000]\tLoss: 0.168355\n",
      "Train Epoch: 8 [41600/60000]\tLoss: 0.014826\n",
      "Train Epoch: 8 [42240/60000]\tLoss: 0.028503\n",
      "Train Epoch: 8 [42880/60000]\tLoss: 0.071637\n",
      "Train Epoch: 8 [43520/60000]\tLoss: 0.071508\n",
      "Train Epoch: 8 [44160/60000]\tLoss: 0.050444\n",
      "Train Epoch: 8 [44800/60000]\tLoss: 0.128268\n",
      "Train Epoch: 8 [45440/60000]\tLoss: 0.183913\n",
      "Train Epoch: 8 [46080/60000]\tLoss: 0.167163\n",
      "Train Epoch: 8 [46720/60000]\tLoss: 0.102954\n",
      "Train Epoch: 8 [47360/60000]\tLoss: 0.069593\n",
      "Train Epoch: 8 [48000/60000]\tLoss: 0.157879\n",
      "Train Epoch: 8 [48640/60000]\tLoss: 0.003529\n",
      "Train Epoch: 8 [49280/60000]\tLoss: 0.038271\n",
      "Train Epoch: 8 [49920/60000]\tLoss: 0.093237\n",
      "Train Epoch: 8 [50560/60000]\tLoss: 0.208411\n",
      "Train Epoch: 8 [51200/60000]\tLoss: 0.131628\n",
      "Train Epoch: 8 [51840/60000]\tLoss: 0.090261\n",
      "Train Epoch: 8 [52480/60000]\tLoss: 0.073786\n",
      "Train Epoch: 8 [53120/60000]\tLoss: 0.130042\n",
      "Train Epoch: 8 [53760/60000]\tLoss: 0.031659\n",
      "Train Epoch: 8 [54400/60000]\tLoss: 0.082396\n",
      "Train Epoch: 8 [55040/60000]\tLoss: 0.037575\n",
      "Train Epoch: 8 [55680/60000]\tLoss: 0.088706\n",
      "Train Epoch: 8 [56320/60000]\tLoss: 0.165956\n",
      "Train Epoch: 8 [56960/60000]\tLoss: 0.097683\n",
      "Train Epoch: 8 [57600/60000]\tLoss: 0.111181\n",
      "Train Epoch: 8 [58240/60000]\tLoss: 0.057778\n",
      "Train Epoch: 8 [58880/60000]\tLoss: 0.125424\n",
      "Train Epoch: 8 [59520/60000]\tLoss: 0.061582\n",
      "Train Epoch: 9 [0/60000]\tLoss: 0.164893\n",
      "Train Epoch: 9 [640/60000]\tLoss: 0.077407\n",
      "Train Epoch: 9 [1280/60000]\tLoss: 0.108273\n",
      "Train Epoch: 9 [1920/60000]\tLoss: 0.049698\n",
      "Train Epoch: 9 [2560/60000]\tLoss: 0.035069\n",
      "Train Epoch: 9 [3200/60000]\tLoss: 0.043728\n",
      "Train Epoch: 9 [3840/60000]\tLoss: 0.083903\n",
      "Train Epoch: 9 [4480/60000]\tLoss: 0.138340\n",
      "Train Epoch: 9 [5120/60000]\tLoss: 0.070592\n",
      "Train Epoch: 9 [5760/60000]\tLoss: 0.021590\n",
      "Train Epoch: 9 [6400/60000]\tLoss: 0.045274\n",
      "Train Epoch: 9 [7040/60000]\tLoss: 0.086156\n",
      "Train Epoch: 9 [7680/60000]\tLoss: 0.026216\n",
      "Train Epoch: 9 [8320/60000]\tLoss: 0.030695\n",
      "Train Epoch: 9 [8960/60000]\tLoss: 0.054126\n",
      "Train Epoch: 9 [9600/60000]\tLoss: 0.068456\n",
      "Train Epoch: 9 [10240/60000]\tLoss: 0.099310\n",
      "Train Epoch: 9 [10880/60000]\tLoss: 0.052382\n",
      "Train Epoch: 9 [11520/60000]\tLoss: 0.084612\n",
      "Train Epoch: 9 [12160/60000]\tLoss: 0.090837\n",
      "Train Epoch: 9 [12800/60000]\tLoss: 0.055373\n",
      "Train Epoch: 9 [13440/60000]\tLoss: 0.051879\n",
      "Train Epoch: 9 [14080/60000]\tLoss: 0.042639\n",
      "Train Epoch: 9 [14720/60000]\tLoss: 0.243360\n",
      "Train Epoch: 9 [15360/60000]\tLoss: 0.130705\n",
      "Train Epoch: 9 [16000/60000]\tLoss: 0.159516\n",
      "Train Epoch: 9 [16640/60000]\tLoss: 0.142749\n",
      "Train Epoch: 9 [17280/60000]\tLoss: 0.074023\n",
      "Train Epoch: 9 [17920/60000]\tLoss: 0.007744\n",
      "Train Epoch: 9 [18560/60000]\tLoss: 0.145094\n",
      "Train Epoch: 9 [19200/60000]\tLoss: 0.098436\n",
      "Train Epoch: 9 [19840/60000]\tLoss: 0.070456\n",
      "Train Epoch: 9 [20480/60000]\tLoss: 0.029686\n",
      "Train Epoch: 9 [21120/60000]\tLoss: 0.044223\n",
      "Train Epoch: 9 [21760/60000]\tLoss: 0.083627\n",
      "Train Epoch: 9 [22400/60000]\tLoss: 0.041114\n",
      "Train Epoch: 9 [23040/60000]\tLoss: 0.186633\n",
      "Train Epoch: 9 [23680/60000]\tLoss: 0.120760\n",
      "Train Epoch: 9 [24320/60000]\tLoss: 0.296852\n",
      "Train Epoch: 9 [24960/60000]\tLoss: 0.110787\n",
      "Train Epoch: 9 [25600/60000]\tLoss: 0.027053\n",
      "Train Epoch: 9 [26240/60000]\tLoss: 0.009881\n",
      "Train Epoch: 9 [26880/60000]\tLoss: 0.147147\n",
      "Train Epoch: 9 [27520/60000]\tLoss: 0.094822\n",
      "Train Epoch: 9 [28160/60000]\tLoss: 0.107138\n",
      "Train Epoch: 9 [28800/60000]\tLoss: 0.033138\n",
      "Train Epoch: 9 [29440/60000]\tLoss: 0.022580\n",
      "Train Epoch: 9 [30080/60000]\tLoss: 0.110754\n",
      "Train Epoch: 9 [30720/60000]\tLoss: 0.111913\n",
      "Train Epoch: 9 [31360/60000]\tLoss: 0.008921\n",
      "Train Epoch: 9 [32000/60000]\tLoss: 0.207582\n",
      "Train Epoch: 9 [32640/60000]\tLoss: 0.117226\n",
      "Train Epoch: 9 [33280/60000]\tLoss: 0.047503\n",
      "Train Epoch: 9 [33920/60000]\tLoss: 0.241504\n",
      "Train Epoch: 9 [34560/60000]\tLoss: 0.167008\n",
      "Train Epoch: 9 [35200/60000]\tLoss: 0.127400\n",
      "Train Epoch: 9 [35840/60000]\tLoss: 0.079654\n",
      "Train Epoch: 9 [36480/60000]\tLoss: 0.141728\n",
      "Train Epoch: 9 [37120/60000]\tLoss: 0.072620\n",
      "Train Epoch: 9 [37760/60000]\tLoss: 0.080869\n",
      "Train Epoch: 9 [38400/60000]\tLoss: 0.068588\n",
      "Train Epoch: 9 [39040/60000]\tLoss: 0.070861\n",
      "Train Epoch: 9 [39680/60000]\tLoss: 0.088649\n",
      "Train Epoch: 9 [40320/60000]\tLoss: 0.084055\n",
      "Train Epoch: 9 [40960/60000]\tLoss: 0.073941\n",
      "Train Epoch: 9 [41600/60000]\tLoss: 0.167977\n",
      "Train Epoch: 9 [42240/60000]\tLoss: 0.027702\n",
      "Train Epoch: 9 [42880/60000]\tLoss: 0.311870\n",
      "Train Epoch: 9 [43520/60000]\tLoss: 0.042705\n",
      "Train Epoch: 9 [44160/60000]\tLoss: 0.144408\n",
      "Train Epoch: 9 [44800/60000]\tLoss: 0.188037\n",
      "Train Epoch: 9 [45440/60000]\tLoss: 0.077042\n",
      "Train Epoch: 9 [46080/60000]\tLoss: 0.035539\n",
      "Train Epoch: 9 [46720/60000]\tLoss: 0.081997\n",
      "Train Epoch: 9 [47360/60000]\tLoss: 0.020776\n",
      "Train Epoch: 9 [48000/60000]\tLoss: 0.072981\n",
      "Train Epoch: 9 [48640/60000]\tLoss: 0.086707\n",
      "Train Epoch: 9 [49280/60000]\tLoss: 0.231848\n",
      "Train Epoch: 9 [49920/60000]\tLoss: 0.061949\n",
      "Train Epoch: 9 [50560/60000]\tLoss: 0.144452\n",
      "Train Epoch: 9 [51200/60000]\tLoss: 0.096958\n",
      "Train Epoch: 9 [51840/60000]\tLoss: 0.055620\n",
      "Train Epoch: 9 [52480/60000]\tLoss: 0.017969\n",
      "Train Epoch: 9 [53120/60000]\tLoss: 0.187861\n",
      "Train Epoch: 9 [53760/60000]\tLoss: 0.081637\n",
      "Train Epoch: 9 [54400/60000]\tLoss: 0.079896\n",
      "Train Epoch: 9 [55040/60000]\tLoss: 0.031933\n",
      "Train Epoch: 9 [55680/60000]\tLoss: 0.028800\n",
      "Train Epoch: 9 [56320/60000]\tLoss: 0.213158\n",
      "Train Epoch: 9 [56960/60000]\tLoss: 0.058491\n",
      "Train Epoch: 9 [57600/60000]\tLoss: 0.055608\n",
      "Train Epoch: 9 [58240/60000]\tLoss: 0.092672\n",
      "Train Epoch: 9 [58880/60000]\tLoss: 0.279397\n",
      "Train Epoch: 9 [59520/60000]\tLoss: 0.091943\n",
      "Train Epoch: 10 [0/60000]\tLoss: 0.004406\n",
      "Train Epoch: 10 [640/60000]\tLoss: 0.084995\n",
      "Train Epoch: 10 [1280/60000]\tLoss: 0.018363\n",
      "Train Epoch: 10 [1920/60000]\tLoss: 0.077367\n",
      "Train Epoch: 10 [2560/60000]\tLoss: 0.168525\n",
      "Train Epoch: 10 [3200/60000]\tLoss: 0.054983\n",
      "Train Epoch: 10 [3840/60000]\tLoss: 0.020951\n",
      "Train Epoch: 10 [4480/60000]\tLoss: 0.086145\n",
      "Train Epoch: 10 [5120/60000]\tLoss: 0.102134\n",
      "Train Epoch: 10 [5760/60000]\tLoss: 0.156029\n",
      "Train Epoch: 10 [6400/60000]\tLoss: 0.142128\n",
      "Train Epoch: 10 [7040/60000]\tLoss: 0.022885\n",
      "Train Epoch: 10 [7680/60000]\tLoss: 0.077459\n",
      "Train Epoch: 10 [8320/60000]\tLoss: 0.045451\n",
      "Train Epoch: 10 [8960/60000]\tLoss: 0.029871\n",
      "Train Epoch: 10 [9600/60000]\tLoss: 0.190913\n",
      "Train Epoch: 10 [10240/60000]\tLoss: 0.041388\n",
      "Train Epoch: 10 [10880/60000]\tLoss: 0.076591\n",
      "Train Epoch: 10 [11520/60000]\tLoss: 0.049561\n",
      "Train Epoch: 10 [12160/60000]\tLoss: 0.077618\n",
      "Train Epoch: 10 [12800/60000]\tLoss: 0.038216\n",
      "Train Epoch: 10 [13440/60000]\tLoss: 0.202401\n",
      "Train Epoch: 10 [14080/60000]\tLoss: 0.076624\n",
      "Train Epoch: 10 [14720/60000]\tLoss: 0.022550\n",
      "Train Epoch: 10 [15360/60000]\tLoss: 0.056849\n",
      "Train Epoch: 10 [16000/60000]\tLoss: 0.039927\n",
      "Train Epoch: 10 [16640/60000]\tLoss: 0.111454\n",
      "Train Epoch: 10 [17280/60000]\tLoss: 0.088212\n",
      "Train Epoch: 10 [17920/60000]\tLoss: 0.165923\n",
      "Train Epoch: 10 [18560/60000]\tLoss: 0.150267\n",
      "Train Epoch: 10 [19200/60000]\tLoss: 0.060551\n",
      "Train Epoch: 10 [19840/60000]\tLoss: 0.028571\n",
      "Train Epoch: 10 [20480/60000]\tLoss: 0.234527\n",
      "Train Epoch: 10 [21120/60000]\tLoss: 0.052732\n",
      "Train Epoch: 10 [21760/60000]\tLoss: 0.107240\n",
      "Train Epoch: 10 [22400/60000]\tLoss: 0.114936\n",
      "Train Epoch: 10 [23040/60000]\tLoss: 0.157701\n",
      "Train Epoch: 10 [23680/60000]\tLoss: 0.009720\n",
      "Train Epoch: 10 [24320/60000]\tLoss: 0.025806\n",
      "Train Epoch: 10 [24960/60000]\tLoss: 0.114925\n",
      "Train Epoch: 10 [25600/60000]\tLoss: 0.147102\n",
      "Train Epoch: 10 [26240/60000]\tLoss: 0.233183\n",
      "Train Epoch: 10 [26880/60000]\tLoss: 0.109247\n",
      "Train Epoch: 10 [27520/60000]\tLoss: 0.054149\n",
      "Train Epoch: 10 [28160/60000]\tLoss: 0.095042\n",
      "Train Epoch: 10 [28800/60000]\tLoss: 0.067657\n",
      "Train Epoch: 10 [29440/60000]\tLoss: 0.055772\n",
      "Train Epoch: 10 [30080/60000]\tLoss: 0.095608\n",
      "Train Epoch: 10 [30720/60000]\tLoss: 0.070402\n",
      "Train Epoch: 10 [31360/60000]\tLoss: 0.083570\n",
      "Train Epoch: 10 [32000/60000]\tLoss: 0.081184\n",
      "Train Epoch: 10 [32640/60000]\tLoss: 0.210013\n",
      "Train Epoch: 10 [33280/60000]\tLoss: 0.044010\n",
      "Train Epoch: 10 [33920/60000]\tLoss: 0.029288\n",
      "Train Epoch: 10 [34560/60000]\tLoss: 0.020526\n",
      "Train Epoch: 10 [35200/60000]\tLoss: 0.106207\n",
      "Train Epoch: 10 [35840/60000]\tLoss: 0.110391\n",
      "Train Epoch: 10 [36480/60000]\tLoss: 0.106282\n",
      "Train Epoch: 10 [37120/60000]\tLoss: 0.151380\n",
      "Train Epoch: 10 [37760/60000]\tLoss: 0.047360\n",
      "Train Epoch: 10 [38400/60000]\tLoss: 0.161431\n",
      "Train Epoch: 10 [39040/60000]\tLoss: 0.055438\n",
      "Train Epoch: 10 [39680/60000]\tLoss: 0.232187\n",
      "Train Epoch: 10 [40320/60000]\tLoss: 0.105629\n",
      "Train Epoch: 10 [40960/60000]\tLoss: 0.021313\n",
      "Train Epoch: 10 [41600/60000]\tLoss: 0.126338\n",
      "Train Epoch: 10 [42240/60000]\tLoss: 0.094345\n",
      "Train Epoch: 10 [42880/60000]\tLoss: 0.024793\n",
      "Train Epoch: 10 [43520/60000]\tLoss: 0.015298\n",
      "Train Epoch: 10 [44160/60000]\tLoss: 0.159848\n",
      "Train Epoch: 10 [44800/60000]\tLoss: 0.070369\n",
      "Train Epoch: 10 [45440/60000]\tLoss: 0.009460\n",
      "Train Epoch: 10 [46080/60000]\tLoss: 0.160004\n",
      "Train Epoch: 10 [46720/60000]\tLoss: 0.026587\n",
      "Train Epoch: 10 [47360/60000]\tLoss: 0.348485\n",
      "Train Epoch: 10 [48000/60000]\tLoss: 0.069834\n",
      "Train Epoch: 10 [48640/60000]\tLoss: 0.082805\n",
      "Train Epoch: 10 [49280/60000]\tLoss: 0.121418\n",
      "Train Epoch: 10 [49920/60000]\tLoss: 0.088839\n",
      "Train Epoch: 10 [50560/60000]\tLoss: 0.120967\n",
      "Train Epoch: 10 [51200/60000]\tLoss: 0.049966\n",
      "Train Epoch: 10 [51840/60000]\tLoss: 0.091951\n",
      "Train Epoch: 10 [52480/60000]\tLoss: 0.035359\n",
      "Train Epoch: 10 [53120/60000]\tLoss: 0.040703\n",
      "Train Epoch: 10 [53760/60000]\tLoss: 0.005876\n",
      "Train Epoch: 10 [54400/60000]\tLoss: 0.036637\n",
      "Train Epoch: 10 [55040/60000]\tLoss: 0.018826\n",
      "Train Epoch: 10 [55680/60000]\tLoss: 0.132228\n",
      "Train Epoch: 10 [56320/60000]\tLoss: 0.056909\n",
      "Train Epoch: 10 [56960/60000]\tLoss: 0.072663\n",
      "Train Epoch: 10 [57600/60000]\tLoss: 0.030766\n",
      "Train Epoch: 10 [58240/60000]\tLoss: 0.095413\n",
      "Train Epoch: 10 [58880/60000]\tLoss: 0.081336\n",
      "Train Epoch: 10 [59520/60000]\tLoss: 0.107296\n",
      "\n",
      "Test set: Avg. loss: 0.0403, Accuracy: 9865/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and test Model 2\n",
    "\n",
    "# Create network\n",
    "model2 = Net2()\n",
    "# Initialize model weights\n",
    "model2.apply(weights_init)\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model2.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "# Get initial performance\n",
    "test(model2)\n",
    "# Train for ten epochs\n",
    "n_epochs = 10\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch, model2)\n",
    "accuracy2 = test(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFbCnAUmTwyx"
   },
   "source": [
    "## III. Results\n",
    "\n",
    "Here we train the CNN model and apply it to the test set. There are 10 epochs in training. There is no validation set here, we simply take the model at the end of the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1730967071191,
     "user": {
      "displayName": "Hsin-En Tsai",
      "userId": "01830060851848219976"
     },
     "user_tz": -480
    },
    "id": "JgAKHjLbqm3S",
    "outputId": "319250d6-f8f3-4972-fe53-4ea6a66abf87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Accuracy: 92.52%\n",
      "Model 2 Accuracy: 98.65%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model 1 Accuracy: {round(float(accuracy1.numpy()),2)}%\")\n",
    "print(f\"Model 2 Accuracy: {round(float(accuracy2.numpy()),2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "executionInfo": {
     "elapsed": 797,
     "status": "ok",
     "timestamp": 1730967071985,
     "user": {
      "displayName": "Hsin-En Tsai",
      "userId": "01830060851848219976"
     },
     "user_tz": -480
    },
    "id": "8hG1l1rSulbg",
    "outputId": "991f1697-0d6e-4b44-961a-626d08fbba94"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-b8ebf8cbff0b>:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.log_softmax(x)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run network on data we got before and show predictions\n",
    "output = model(example_data)\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(10):\n",
    "  plt.subplot(5,5,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Prediction: {}\".format(\n",
    "    output.data.max(1, keepdim=True)[1][i].item()))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "executionInfo": {
     "elapsed": 1081,
     "status": "ok",
     "timestamp": 1730967073065,
     "user": {
      "displayName": "Hsin-En Tsai",
      "userId": "01830060851848219976"
     },
     "user_tz": -480
    },
    "id": "zUHLA7qru5cQ",
    "outputId": "1e798ef8-48b0-424f-a244-c90a5629f8fa"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run network on data we got before and show predictions\n",
    "output = model2(example_data)\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(10):\n",
    "  plt.subplot(5,5,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Prediction: {}\".format(\n",
    "    output.data.max(1, keepdim=True)[1][i].item()))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0w7iym1T2QY"
   },
   "source": [
    "# IV. Conclusion and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unftkMFUfOz-"
   },
   "source": [
    "**Discussion**\n",
    "\n",
    "　　這次對 MNIST 數字分類的作業中，得到的結果是 2D 網絡模型（model2）的表現比 1D 網絡模型（model）的表現好。\n",
    "\n",
    "1D model 的 Test set: Avg. loss: 0.2793, Accuracy: 9252/10000 (93%)\n",
    "\n",
    "2D model 的 Test set: Avg. loss: 0.0403, Accuracy: 9865/10000 (99%)。\n",
    "\n",
    "從結果可看出相較於 1D 模型（Accuracy 約為93%）， 2D 模型的預測（Accuracy 約為 99%）更為準確、2D 模型的損失（0.0403）也比 1D 模型的損失（0.2793）少許多，且 2D 模型通常誤分類的情況較少。這兩個模型之間的主要區別可能在於如何處理空間信息。2D 網絡結構能有效捕捉圖像中的空間模式，例如數字的邊緣和形狀，使其更適合處理像 MNIST 的圖像數據。相反，1D 模型可能難以提取這些空間特徵，從而導致較低的準確度。\n",
    "\n",
    "　　除了網絡架構之外，訓練輪次的數量也是提高準確度和減少損失的關鍵。增加 epoch 能讓模型有更多的迭代機會去學習數據中的模式，有助於提高準確度並減少平均損失。然而，在某個點之後，增加 epoch 可能會導致過擬合，這時模型在訓練數據上的表現很好，但在未見過的測試數據上表現較差。為了平衡這一點，利用像是 Early Stopping、Regularization 或驗證損失監控等方法，可以有效避免過擬合，同時確保模型達到預期的準確度。\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "　　從這次作業可以看出模型架構和訓練時間在圖像分類任務中達到高準確度的重要性。由於能夠有效利用數據中的空間關係，2D 模型在 MNIST 分類中表現優於 1D 模型。為了進一步提高性能，調整訓練輪次的數量並實施防止過擬合可能會有效。未來的改進可以透過優化 hyperparameters（如 learning rate、批量大小）以及加入數據增強（Data Augmentation）等方法，進一步提升模型的準確度並減少平均損失。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
